<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>论文精读2：Training-Free Layout Control with Cross-Attention Guidance | 诺亚方舟</title><meta name="author" content="弘树"><meta name="copyright" content="弘树"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="1 Abstract最近基于扩散的生成器可以从文本提示生成高质量的图像。然而，他们经常忽略指定组合空间布局的文本指令。我们提出了一种实现鲁棒布局控制的简单方法，而不需要对图像生成器进行训练或微调。我们的技术操纵模型使用的交叉注意力层来连接文本和视觉信息，并在给定的期望方向上引导生成，例如用户指定的布局。 为了确定如何最好地指导注意力，我们研究了注意力图的作用，并探索两种替代策略：前向和后向指导。我">
<meta property="og:type" content="article">
<meta property="og:title" content="论文精读2：Training-Free Layout Control with Cross-Attention Guidance">
<meta property="og:url" content="http://zhouzimu.top/2024/08/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB2%EF%BC%9ATraining-Free-Layout-Control-with-Cross-Attention-Guidance/index.html">
<meta property="og:site_name" content="诺亚方舟">
<meta property="og:description" content="1 Abstract最近基于扩散的生成器可以从文本提示生成高质量的图像。然而，他们经常忽略指定组合空间布局的文本指令。我们提出了一种实现鲁棒布局控制的简单方法，而不需要对图像生成器进行训练或微调。我们的技术操纵模型使用的交叉注意力层来连接文本和视觉信息，并在给定的期望方向上引导生成，例如用户指定的布局。 为了确定如何最好地指导注意力，我们研究了注意力图的作用，并探索两种替代策略：前向和后向指导。我">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://github.com/silent-chen/layout-guidance/raw/gh-page/resources/teaser.png?raw=true">
<meta property="article:published_time" content="2024-08-15T05:27:59.000Z">
<meta property="article:modified_time" content="2024-11-14T03:08:39.000Z">
<meta property="article:author" content="弘树">
<meta property="article:tag" content="layout-guidance">
<meta property="article:tag" content="论文精读">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/silent-chen/layout-guidance/raw/gh-page/resources/teaser.png?raw=true"><link rel="shortcut icon" href="/img/favicon.jpg"><link rel="canonical" href="http://zhouzimu.top/2024/08/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB2%EF%BC%9ATraining-Free-Layout-Control-with-Cross-Attention-Guidance/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":1,"unescape":true,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: 弘树","link":"链接: ","source":"来源: 诺亚方舟","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '论文精读2：Training-Free Layout Control with Cross-Attention Guidance',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-11-14 11:08:39'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/myStyle.css"><link rel="stylesheet" href="/css/tianli_gpt.css"><meta name="generator" content="Hexo 6.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="loading-box"><div class="wizard-scene"><div class="wizard-objects"><div class="wizard-square"></div><div class="wizard-circle"></div><div class="wizard-triangle"></div></div><div class="wizard"><div class="wizard-body"></div><div class="wizard-right-arm"><div class="wizard-right-hand"></div></div><div class="wizard-left-arm"><div class="wizard-left-hand"></div></div><div class="wizard-head"><div class="wizard-beard"></div><div class="wizard-face"><div class="wizard-adds"></div></div><div class="wizard-hat"><div class="wizard-hat-of-the-hat"></div><div class="wizard-four-point-star --first"></div><div class="wizard-four-point-star --second"></div><div class="wizard-four-point-star --third"></div></div></div></div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/pic_editor_1635545191.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">123</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">45</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 其他</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-asterisk"></i><span> 归档</span></a></li><li><a class="site-page child" href="/update/"><i class="fa-fw fas fa-archive"></i><span> 更新日志</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url('https://github.com/silent-chen/layout-guidance/raw/gh-page/resources/teaser.png?raw=true')"><nav id="nav"><span id="blog-info"><a href="/" title="诺亚方舟"><span class="site-name">诺亚方舟</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 其他</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-asterisk"></i><span> 归档</span></a></li><li><a class="site-page child" href="/update/"><i class="fa-fw fas fa-archive"></i><span> 更新日志</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">论文精读2：Training-Free Layout Control with Cross-Attention Guidance</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2024-08-15T05:27:59.000Z" title="发表于 2024-08-15 13:27:59">2024-08-15</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%A7%91%E7%A0%94/">科研</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">9.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>30分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="论文精读2：Training-Free Layout Control with Cross-Attention Guidance"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1 Abstract"></a>1 Abstract</h1><p>最近基于扩散的生成器可以从文本提示生成高质量的图像。然而，他们经常忽略指定组合空间布局的文本指令。我们提出了一种实现鲁棒布局控制的简单方法，而不需要对图像生成器进行训练或微调。我们的技术操纵模型使用的交叉注意力层来连接文本和视觉信息，并在给定的期望方向上引导生成，例如用户指定的布局。</p>
<p>为了确定如何最好地指导注意力，我们研究了注意力图的作用，并探索两种替代策略：前向和后向指导。我们在三个基准上彻底评估了我们的方法，并对两种策略进行了比较分析，证明了后向引导与前向引导以及先前的工作相比的优越性。我们通过将布局引导扩展到编辑真实图像布局和上下文等应用程序来进一步演示布局引导的通用性。</p>
<hr>
<h1 id="2-Introduction"><a href="#2-Introduction" class="headerlink" title="2 Introduction"></a>2 Introduction</h1><p>生成 AI 是过去几年出现的最具创新性的技术之一。在计算机视觉中，新的文本到图像生成方法，如<code>DALL-E</code>[37]、<code>Imagen</code>[43]和<code>Stable Diffusion</code>[39]，已经证明机器能够生成足够高质量的图像，用于许多应用，提高专业艺术家和非专业人士的生产力。</p>
<p>然而，尽管取得了这一成功，但图像生成的许多实际应用，特别是在专业环境中，都需要这种方法所缺乏的高水平控制。基于语言的图像生成器中的规范是文本的；虽然文本可以利用庞大的高级概念库，但它不是表达图像中细粒度视觉细微差别的好工具。具体来说，文本往往不足以描述作文的确切布局。</p>
<p>事实上，如之前的工作[16]所示，最先进的图像生成器很难正确解释通过文本指定的简单布局指令。例如，当用“猫左边的狗”这样的短语提示此类模型时，“猫左边”的关系并不总是在生成的图像中准确描绘出来。事实上，这种性质的提示往往会导致模型产生错误的语义，例如猫狗混合体的图像。这种限制因不寻常的组合而加剧，例如“房子顶上的马”，这超出了模型在训练过程中观察到的典型组合。</p>
<p>我们的工作更好地理解了这一局限性，并提出了一种克服它的机制。为此，我们介绍了一种方法，该方法无需对图像生成器进行进一步训练即可实现布局控制，同时仍能保持生成图像的质量。</p>
<p>我们注意到，虽然布局不能通过文本提示轻松控制，但可以直接干预交叉注意力层，通过用户指定的输入（如边界框）将生成引导到选择的方向，我们称之为布局指导。我们考虑并比较了这种干预的两种替代策略：“正向指导”和“反向指导”。</p>
<ul>
<li>正向引导直接使交叉注意力层偏移到所需模式中的激活，让模型通过迭代应用其去噪步骤来结合引导。</li>
<li>我们的主要贡献是反向引导，它使用反向传播来更新图像延迟，通过能量最小化来匹配所需的布局。</li>
</ul>
<p>虽然布局控制已经受到了一些关注，但一些方法遵循了正向范式[2, 45]，我们表明反向引导是一种更有效的机制。然后，我们的第二个贡献是深入研究了图像生成过程中影响布局的因素，揭示了正向制导的缺点，并讨论了反向制导如何解决这些问题。我们发现，虽然不同概念与其视觉范围之间存在直观的相关性，但这种相关性比人们想象的要微妙得多，而且，也许与直觉相反，即使是提示中的特殊标记（开始标记和填充标记）也有助于塑造布局。</p>
<p>最后，我们展示了我们的反向引导优于现有方法，并无缝集成到真实图像布局编辑等应用程序中。</p>
<hr>
<h1 id="3-Related-Work"><a href="#3-Related-Work" class="headerlink" title="3 Related Work"></a>3 Related Work</h1><h2 id="3-1-文本到图像生成"><a href="#3-1-文本到图像生成" class="headerlink" title="3.1 文本到图像生成"></a>3.1 文本到图像生成</h2><p>多年来，生成对抗网络（<code>GANs</code>）[17]一直是从文本提示生成图像的主要方法[38, 48, 51, 56-58]。也考虑了文本的替代表示，如场景图[25]。</p>
<p>最近，研究重点转移到文本条件自回归[10, 14, 37, 55]和扩散模型[18, 32, 36, 39, 43]上，在生成具有出色保真度的图像方面取得了令人印象深刻的结果，同时避免了常见的<code>GAN</code>陷阱，如训练不稳定和模式崩溃[9]。数据规模[44]和变压器模型[35]的大小和功能的大幅增加在实现这一转变方面发挥了至关重要的作用。通常，这些模型被设计为接受文本提示作为输入，这可能会对准确传达图像的所有细节构成挑战。</p>
<p>当提示时间较长或描述非典型场景时，这个问题会加剧。最近的研究表明，无分类器指导[21]在提高带有输入提示的生成可信度方面是有效的。其他则专注于提高组合性，例如，通过将多个扩散模型与不同的运算符组合[30]，以及属性绑定[5, 13]。</p>
<h2 id="3-2-图像生成中的布局控制"><a href="#3-2-图像生成中的布局控制" class="headerlink" title="3.2 图像生成中的布局控制"></a>3.2 图像生成中的布局控制</h2><p>具有空间条件的图像生成与布局控制密切相关，通常通过边界框或语义图完成[12, 33, 46, 47, 52, 60]。这些方法不使用文本提示，而是依赖于闭集词汇表来生成图像，即训练分布的标签（例如COCO[29]）。最近的图像文本模型，如CLIP[35]，现在能够扩展到开放词汇表。然而，仅通过文本传达一篇作文的精确布局仍然具有挑战性；即便如此，图像生成器的空间保真度也极其有限[16]。因此，还考虑了文本和布局的联合条件[14, 20, 24]以及从文本预测布局[22]。</p>
<p>最近的研究[1, 2, 4, 6, 27, 45, 50, 53]提出用空间调节来扩展最先进的稳定扩散[39]。<code>GLIGEN</code>[27]和<code>ReCo</code>[53]分别使用门控自我注意层和额外的区域标记对扩散模型进行微调。其他作品[2, 4, 6, 45, 50]遵循无训练的方法。<code>MultiDiffusion</code>[4]采用了[30]中的思想，结合了掩蔽噪声。<code>eDiffI</code>[2]和<code>HFG</code>[45]与我们的前瞻性指导有相似的想法，直接干预交叉注意力。然而，他们忽视了特殊代币在这一过程中的重要性。在我们工作的同时，<code>ZestGuide</code>[6]和<code>BoxDiff</code>[50]提出计算交叉注意力损失以实现布局控制，这更接近我们的反向引导。与之前的工作不同，我们使用了一个不依赖于用户提供的精确分割掩模的目标函数，并对影响布局的因素进行了深入分析，从而对正向和反向策略的行为进行了分析。最后，在扩散的基础上，最近的一些作品展示了从各种其他调节信号[3, 23, 59]（如深度或边缘图）生成可控图像。</p>
<h2 id="3-3-基于扩散的图像编辑"><a href="#3-3-基于扩散的图像编辑" class="headerlink" title="3.3 基于扩散的图像编辑"></a>3.3 基于扩散的图像编辑</h2><p>上述大多数方法缺乏控制或编辑已生成图像的能力，甚至缺乏编辑真实图像的能力。例如，简单地更改原始提示中的单词通常会导致完全不同的生成。这可以通过为感兴趣的对象提供或生成掩码来规避[7, 32]。Prompt to Prompt[19]利用大多数最先进架构中存在的交叉注意力层将单词标记与生成图像的空间布局联系起来的事实，通过简单的基于文本的编辑来解决这个问题。基于文本的图像编辑也可以通过单图像模型微调来实现[26,49]。然而，这些方法虽然在语义编辑实体方面取得了成功，但只能就地应用这些编辑，而不允许编辑空间布局本身。</p>
<hr>
<h1 id="4-Method"><a href="#4-Method" class="headerlink" title="4 Method"></a>4 Method</h1><p>我们考虑了布局引导文本到图像生成的问题。基于文本的图像生成器允许从条件分布$p(x|y)$中采样图像$x\in R^3×H\times W$，其中$y$是语言描述。</p>
<p>给定一个现成的生成器，我们希望在不进行进一步训练或微调的情况下，将其输出调整为与生成的组合的所需布局相匹配。换句话说，我们的目标是研究预训练的文本到图像生成器是否可以在推理过程中遵守用户指定的布局，而无需经过显式布局条件训练。在最简单的情况下，给定文本提示$y$、文本提示中单词$y_i$的索引$i$和边界框$B$，我们想生成一个在$B$内包含$y_i$的图像$x$，本质上是修改生成器，使其通过额外的控件从新的分布$p(x|y,B,i)$中采样。</p>
<h2 id="4-1-准备工作：Stable-Diffusion"><a href="#4-1-准备工作：Stable-Diffusion" class="headerlink" title="4.1 准备工作：Stable Diffusion"></a>4.1 准备工作：Stable Diffusion</h2><p>我们首先简要回顾了Stable Diffusion(SD)[39]的技术细节，这是一个公开可用的、最先进的文本到图像生成器，代表了基于扩散的重要图像生成器类别[37, 39, 43]。SD 由图像编码器和解码器、文本编码器和在潜在空间中运行的去噪网络组成。</p>
<p>文本编码器$Y&#x3D; \phi(y) $将输入提示映射到固定维度$ Y\in R^{N \times M}$的张量。这是通过将起始符号 <code>[SoT]</code> 添加到$y$并在最后附加$N-|y|-1$个填充符号<code>[EoT]</code>来工作，总共获得$N$个符号。然后，实现为大型语言模型<code>(LLM)</code>的函数$\phi$将填充的单词序列作为输入，并生成对应的令牌序列$Y_i \in R^M$，其中$ i ∈ {1,…,N }$作为输出。</p>
<p>虽然对我们的讨论并不重要，但 SD 的编码网络$h$将图像$x$映射到相应的潜在代码$z&#x3D;h(x)\in R^{4 \times \frac{H}{s} \times \frac{W}{s}}$，其中 $s$ 划分$H$和$W$。函数$h$是一个具有左逆$h^*$的自动编码器，使得$x &#x3D; h^∗ ◦ h(x)$。该组件的主要目的是用建模$p(z | y)$的问题替换建模$p(x | y)$的问题，降低空间分辨率$s$倍。</p>
<p>SD的一个关键组成部分是迭代条件去噪网络D。该网络经过训练以输出潜在代码 z 的条件样本 $z ∼ p(z | y)$。它的目的是以噪声样本$z_t&#x3D;\alpha_tz+\sqrt{1-\alpha_t}\epsilon_t$为输入，其中$\epsilon_t$为正态分布噪声，$\alpha_t$为递减序列，从$α_0≈1$到$α_T≈0$，表示噪声调度。然后，网络 D 返回噪声样本 $z_t$ 的估计：$D(z_t, y, t) ≈ ε_t$。</p>
<p>为了对图像进行采样，首先采样正态分布的$z_T$并迭代地应用 D 以获得中间代码 $z_{T-1},…, z_1, z_0 ≈ z$。最后，$z$通过图像解码器$x &#x3D; h∗(z)$转换回图像。</p>
<p>SD 架构的最后一个方面与我们的工作相关。虽然有几个设计选择使网络 D 在实践中运行良好，但我们感兴趣的机制是交叉注意力，它连接视觉和文本信息并允许生成过程以文本为条件。每个交叉注意力层以中间特征张量$z^{(γ)} ∈ R^{C× \frac{H}{r} × \frac{W}{r}}$作为输入，其中$γ$是网络中相关层的索引，$r$是定义该表示级别的空间分辨率的比例因子。交叉注意力图$A(γ)$关联每个空间位置$u ∈ {1,…,\frac{H}{r}} × {1,…,\frac{W}{r}}$到由 $i ∈ {1,…,N}$：</p>
<p><img src="/2024/08/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB2%EF%BC%9ATraining-Free-Layout-Control-with-Cross-Attention-Guidance/image.png"></p>
<p>其中值$V^{(γ)}_i$和键$K^{(γ)}_i$是文本编码器提供的令牌嵌入$Y_i$的线性变换，$Q^{(γ)}$是$z^{(γ)}$的线性变换，$a^{(γ)}_u$是交叉注意力层的输出。</p>
<h2 id="4-2-布局指导"><a href="#4-2-布局指导" class="headerlink" title="4.2 布局指导"></a>4.2 布局指导</h2><p>像 SD 这样的文本到图像生成器很难准确解释文本提供的布局指令。因此，我们引入了一种方法，通过从具有附加控制的分布$  p(x |y, B, i)  $中采样来指导生成过程期间的布局。例如，对应于所选文本标记$y_i$的用户指定的边界框$B$，这可以通过操纵架构中某些交叉注意力层中的注意力响应来实现。</p>
<p>已经表明，交叉注意力层调节生成图像的空间布局[19]。具体来说，$A_{ui}^{(γ)}$ 决定了层$γ$中每个位置$u$与$N$个文本标记$y_i$中的每一个相关联的强度。由于每个空间位置$u$的关联强度$\sum_{i&#x3D;1}^{N} A_{u i}^{(\gamma)}&#x3D;1$的总和，因此不同的标记可以被视为位置“竞争”。为了使用对应于令牌$y_i$的边界框$B$来控制图像布局，可以偏向注意力，使得目标框内的位置$u∈B$与$y_i$密切相关（而其他位置则不是）。如下所述，这可以在不微调图像生成器或训练附加层的情况下完成。</p>
<p>接下来，我们对两种策略进行了全面调查，以实现无训练布局控制：前向和后向引导（图 2）。虽然最近在最近的工作 [2, 45] 中已经讨论了前向引导的实例，但我们在此形式化了这种方法，确定了它的局限性，并提出了后向引导作为更有效的替代方案。</p>
<h3 id="4-2-1-前向指导"><a href="#4-2-1-前向指导" class="headerlink" title="4.2.1 前向指导"></a>4.2.1 前向指导</h3><p>在前向引导中，边界框B被表示为平滑窗口函数$g_{u}^{(\gamma)}$，它等于盒子内的常数$c&gt;0$，并且很快下降到外部的零。</p>
<p><img src="/2024/08/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB2%EF%BC%9ATraining-Free-Layout-Control-with-Cross-Attention-Guidance/image_G1S4K8zeM4.png"></p>
<blockquote>
<p>图2：两种布局引导策略概述。所选单词标记的交叉注意力图用红色边框标记。在前向引导中，单词、开始和结束标记的交叉注意力图在空间上是有偏差的。在后向引导中，我们改为计算损失函数并在推理过程中执行反向传播以优化潜在。</p>
</blockquote>
<p>我们对窗口函数进行缩放，使$∥g(γ)∥_1&#x3D;1$。然后，我们通过用以下公式替换交叉注意图来偏置交叉注意图：</p>
<p>$$<br>A_{u i}^{(\gamma)} \leftarrow(1-\lambda) A_{u i}^{(\gamma)}+\lambda g_{u}^{(\gamma)} \sum_{v} A_{v i}^{(\gamma)}<br>$$</p>
<p>其中$λ ∈ [0, 1]$定义了干预的强度。在实践中，我们对上述公式的右侧进行归一化，沿文本标记维度使用<code>softmax</code>函数，保持逐像素注意的总和等于1。注意上述公式只操纵第$i$个token的交叉注意映射$A_{:, i}^{(\gamma)}$，并且窗口由质量$\sum_{v} A_{v i}^{(\gamma)}$加权，使后者保持不变。</p>
<p>该干预应用于所选层$γ∈Γ$上去噪网络$D$的多次迭代。这意味着每个选定层计算的激活图按照等式独立修改。</p>
<p>一个关键的分析表明，前向引导是一种简单的方法，它受到固有约束的影响，阻碍了其提供有效布局控制的能力。正如我们在第 3.3 节中讨论的那样，这主要是由于在生成过程中影响布局的各种因素，包括文本标记之间的空间依赖性和初始噪声中的空间信息“隐藏”。</p>
<h3 id="4-2-2-反向指导"><a href="#4-2-2-反向指导" class="headerlink" title="4.2.2 反向指导"></a>4.2.2 反向指导</h3><p>为了解决前向引导的缺点，我们引入了一种替代机制，我们称之为反向引导。我们不是在反向引导中直接操纵注意力图，而是通过引入能量函数来偏置注意力：</p>
<p>$$<br>E\left(A^{(\gamma)}, B, i\right)&#x3D;\left(1-\frac{\sum_{u \in B} A_{u i}^{(\gamma)}}{\sum_{u} A_{u i}^{(\gamma)}}\right)^{2}<br>$$</p>
<p>优化该函数鼓励第$i$个标记的交叉注意力图获得$B$指定的区域内的更高值。具体来说，在降噪器$D$的每个应用中，当评估层$  γ ∈ Γ  $时，上述公式的损失的梯度通过反向传播计算以更新潜在$z_{t}\left(\equiv z_{t}^{(0)}\right)$：</p>
<p>$$<br>z_{t} \leftarrow z_{t}-\sigma_{t}^{2} \eta \nabla_{z_{t}} \sum_{\gamma \in \Gamma} E\left(A^{(\gamma)}, B, i\right)<br>$$</p>
<p>其中$  η &gt; 0  $是控制引导强度的比例因子，$\sigma_{t}&#x3D;\sqrt{\left(1-\alpha_{t}\right) &#x2F; \alpha_{t}}$。通过更新潜在的，所有标记的交叉注意力图受到后向指导的间接影响。为了生成图像，我们在梯度更新和去噪步骤之间交替。</p>
<h2 id="4-3-分析与讨论"><a href="#4-3-分析与讨论" class="headerlink" title="4.3 分析与讨论"></a>4.3 分析与讨论</h2><p>接下来，我们详细介绍了前向和反向策略之间的比较分析。为了激发反向指导和理解其有效性，我们阐明了所有标记的重要性以及初始噪声在生成过程中塑造布局的影响。</p>
<h3 id="4-3-1-Word-Tokens"><a href="#4-3-1-Word-Tokens" class="headerlink" title="4.3.1 Word Tokens"></a>4.3.1 Word Tokens</h3><p>一个重要的考虑因素是，由于自注意力，文本编码器在处理提示时融合了来自不同单词的信息。这导致“语义重叠”：来自一个token的信息由另一个token编码。换句话说，文本嵌入同时捕获特定于单词和上下文信息，例如主谓宾依赖关系。然后通过交叉注意力层将这种重叠从文本编码器转移到扩散过程中，从而产生空间重叠。</p>
<p><img src="/2024/08/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB2%EF%BC%9ATraining-Free-Layout-Control-with-Cross-Attention-Guidance/image_O4idCETNVb.png"></p>
<blockquote>
<p>图3：前向和后向引导过程中的交叉注意力图。不同单词之间的空间依赖性对前向引导产生负面影响，而后向引导软鼓励所有依赖标记匹配所需的布局。</p>
</blockquote>
<p>图3中的示例说明了不同单词的交叉注意力图中的这种重叠。它还显示了为短语“两个攀爬者”提供空间条件时前向和后向引导的行为。很明显，条件短语的注意力图与其他词（“climbing”、“a”）的空间依赖关系之间的不匹配导致前向指导忽略布局条件。相反，后向引导在必要时间接驱动所有注意力图朝向布局条件，因为它作用于潜在部分。</p>
<h3 id="4-3-2-Special-Tokens"><a href="#4-3-2-Special-Tokens" class="headerlink" title="4.3.2 Special Tokens"></a>4.3.2 Special Tokens</h3><p>另一个关键发现是<code>[SoT]</code>和<code>[EoT]</code>标记的交叉注意力图与输入文本中的内容词不对应，仍然携带显着的语义和布局信息。</p>
<p><img src="/2024/08/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB2%EF%BC%9ATraining-Free-Layout-Control-with-Cross-Attention-Guidance/image_d9lBXNSVtJ.png"></p>
<blockquote>
<p>图4：生成过程中不同文本提示的交叉注意力图。表明开始<code>[SoT]</code>和填充<code>[EoT]</code>标记携带丰富的语义和布局信息。</p>
</blockquote>
<p>如图 4 所示，<code>[EoT]</code>标记的交叉注意力图对应于生成图像中的显着区域，即通常是文本提示中单个语义实体的并集。<code>[SoT]</code>的行为与<code>[EoT]</code>互补，强调背景。为了使前向引导有效，因此不仅需要干预选定的内容标记，还需要干预特殊内容标记。我们使用输入框的并集作为<code>[EoT]</code>的指导，对<code>[SoT]</code>则相反。然而，我们凭经验发现，这有时会导致过于激进的指导，这会损害图像保真度。另一方面，后向引导不会受到这些缺点的影响，因为它进行潜在的优化，我们在补充中进一步讨论这一点。</p>
<h3 id="4-3-3-Initial-Noise"><a href="#4-3-3-Initial-Noise" class="headerlink" title="4.3.3 Initial Noise"></a>4.3.3 Initial Noise</h3><p>最后，扩散过程的初始噪声在塑造图像的布局方面起着重要作用。我们凭经验观察到噪声包含内在布局；例如，当使用相同的种子用“狗的图像”和“猫的图像”等短语提示模型时，它会生成布局一致的图像，将狗和猫放在相同的位置。我们在补充中提供了示例，具有接近用户给出的固有布局的初始噪声更容易优化并导致更高的保真度。</p>
<p>因此，选择与所需布局一致的噪声模式可以进一步提高指导的有效性。在后向引导中，应用于交叉注意力图的损失实际上可以加倍作为初始噪声选择的度量。具体来说，我们对不同的噪声模式进行采样并评估方程。&#x20;</p>
<p>$$<br>E\left(A^{(\gamma)}, B, i\right)&#x3D;\left(1-\frac{\sum_{u \in B} A_{u i}^{(\gamma)}}{\sum_{u} A_{u i}^{(\gamma)}}\right)^{2}<br>$$</p>
<p>上述公式在对几个步骤应用后向引导后，这使我们能够选择最佳对齐的初始噪声。有关详细结果，请参阅补充材料。</p>
<h3 id="4-3-4-Forward-vs-Backward"><a href="#4-3-4-Forward-vs-Backward" class="headerlink" title="4.3.4 Forward vs Backward"></a>4.3.4 Forward vs Backward</h3><p>总之，前向和后向引导使用不同的机制来操纵交叉注意力。前向引导直接修改交叉注意力以符合规定的模式，这对于许多去噪迭代重复“强制”。虽然它不会产生任何额外的计算成本，但它很难提供对布局的稳健控制，因为非引导标记可能会导致生成偏离所需模式。相比之下，后向引导使用损失函数来评估注意力是否遵循所需的模式。虽然比前向引导慢，但后向引导更加细化，因为它间接鼓励所有标记（引导和非引导标记）通过潜在更新遵守布局。</p>
<h2 id="4-4-真实图像布局编辑"><a href="#4-4-真实图像布局编辑" class="headerlink" title="4.4 真实图像布局编辑"></a>4.4 真实图像布局编辑</h2><p>布局指导可以与建立在基于扩散的图像生成器的其他技术相结合。我们证明这对于真实图像编辑的任务。为此，我们将后向引导合并到两种方法中，这两种方法通常用于给定真实图像的扩散模型的个性化，即文本反转 (TI) [15] 和 <code>Dreambooth</code>[42]。TI通过优化概念的可学习文本标记$⟨∗⟩$，将现有的图像生成器扩展为给定一个或几个图像的新概念作为示例。<code>Dreambooth</code>试图通过微调预训练的文本到图像模型来捕获特定主题的外观，其中有几个图像可用。然后，可以生成学习概念的新图像。</p>
<p>这两种方法都不支持对新生成的图像进行局部空间控制；它们的编辑通常是全局和语义的。为此，我们对<code>Dreambooth-finetuned</code>模型和<code>TI-optimized</code>令牌作为提示的一部分应用反向指导。这使我们能够控制生成图像的布局，同时保留由$⟨∗⟩$表示的原始对象的身份。</p>
<hr>
<h1 id="5-Experiments"><a href="#5-Experiments" class="headerlink" title="5 Experiments"></a>5 Experiments</h1><p>在本节中，我们评估了我们的无训练布局指导方法，定量比较前向和后向引导的变体，并在三个基准上提供与之前和并发工作的比较。</p>
<h2 id="5-1-实验环境"><a href="#5-1-实验环境" class="headerlink" title="5.1 实验环境"></a>5.1 实验环境</h2><h3 id="5-1-1-实现细节"><a href="#5-1-1-实现细节" class="headerlink" title="5.1.1 实现细节"></a>5.1.1 实现细节</h3><p>我们利用在<code>LAION-5B</code>数据集[44]上训练的稳定扩散(SD) V-1.5[39]作为默认的预训练图像生成器，如果没有指定。有关架构和噪声调度器的详细描述，请参阅补充。</p>
<p>$$<br>A_{u i}^{(\gamma)} \leftarrow(1-\lambda) A_{u i}^{(\gamma)}+\lambda g_{u}^{(\gamma)} \sum_{v} A_{v i}^{(\gamma)}<br>$$</p>
<p>对于前向指导，我们将上述公式应用于扩散过程的前40步去噪网络的每一层，设$λ &#x3D; 0.8$。对于后向指导，我们计算中块交叉注意图和去噪网络上采样分支的第一个块(U-Net[41])的损失，因为我们发现这是平衡控制和保真度的最佳设置。我们默认设置$η &#x3D; 30$，但发现$  30\sim50  $之间的值在大多数设置中运行良好。由于生成图像的布局通常在推理的早期阶段建立，因此在扩散过程的初始 10 步中执行反向指导，并在每一步重复 5 次。</p>
<h3 id="5-1-2-评估基准"><a href="#5-1-2-评估基准" class="headerlink" title="5.1.2 评估基准"></a>5.1.2 评估基准</h3><p>我们在三个基准上定量评估我们的方法：<code>VISOR</code>[16]、<code>COCO 2014</code>[29]和<code>Flickr30K</code>实体[34,54]。我们讨论了 supp 中数据集使用伦理问题。<code>VISOOR</code>提出了量化文本到图像模型空间理解能力的指标。对于<code>COCO 2014</code>，我们遵循先前工作 [4] 采用的相同设置，它每张图像仅使用注释对象的子集。最后，我们引入 <code>Flickr30K</code>实体数据集作为评估布局控制的另一个基准，因为它包含具有视觉基础的图像-标题对，补充材料中提供了所有基准和指标的详细信息。</p>
<h2 id="5-2-前向与后向指导"><a href="#5-2-前向与后向指导" class="headerlink" title="5.2 前向与后向指导"></a>5.2 前向与后向指导</h2><blockquote>
<p>表 1：前向 (<code>FG</code>) 和后向 (<code>BG</code>) 策略的比较，包括噪声选择 (NS)。<code>FG</code>∗：前向引导包括 <code>[SoT]</code> 和 <code>[EOT]</code> 标记。我们随机抽取了 1000 个文本提示，并基于<code>VISOOR</code>[16] 计算指标。</p>
</blockquote>
<p><img src="/2024/08/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB2%EF%BC%9ATraining-Free-Layout-Control-with-Cross-Attention-Guidance/image_xQP02vNwdu.png"></p>
<p>首先，我们使用具有 1000 个随机选择的文本样本的<code>VISOR</code>协议比较表 1 中的两种不同引导模式（前向和后向）。前向引导的最大优点是计算开销可以忽略不计，从而导致更快的推理时间。然而，我们观察到，与 (unguided) SD 相比，前向引导并没有显着提高对象精度 (<code>OA</code>)，而后向机制产生明显更高的 <code>OA</code>。在评估生成的空间关系（VISOR 条件&#x2F;无条件指标）方面，前向和后向引导都获得了比 SD 基线更好的结果。我们还发现，包含 <code>[SoT]</code> 和 <code>[EoT]</code> 标记改进了前向引导，这证实了我们在第 4.3 节中的分析和见解，但后向引导仍然实现了卓越的性能。最后，使用后向引导进行噪声选择在所有指标上都提供了显着提升。</p>
<p><img src="/2024/08/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB2%EF%BC%9ATraining-Free-Layout-Control-with-Cross-Attention-Guidance/image_-azLfniKyL.png"></p>
<blockquote>
<p>图 5：前向和后向引导之间的比较，包括开始和结束标记的指导。</p>
</blockquote>
<p>我们在图 5 中提供了前向和后向机制的定性比较，包括特殊标记对前向引导的影响。后向引导在生成的对象和输入边界框之间实现了更好的对齐，它还有助于解决扩散模型中生成的图像偶尔会省略对象的问题。</p>
<h2 id="5-3-与先前工作的比较"><a href="#5-3-与先前工作的比较" class="headerlink" title="5.3 与先前工作的比较"></a>5.3 与先前工作的比较</h2><blockquote>
<p>表 2：基于 VISOR [16] 协议的后向引导（我们的）与文本到图像生成模型的比较。</p>
</blockquote>
<p><img src="/2024/08/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB2%EF%BC%9ATraining-Free-Layout-Control-with-Cross-Attention-Guidance/image_wxEulC8HVj.png"></p>
<p>在表 2 中，我们将我们的方法与不使用布局控制的文本到图像生成方法进行比较。我们注意到比较是公平的，因为在此设置 (VISOR) 中，指导不需要手动用户输入（见补充）。我们的方法在 <code>VISORcond</code>指标下表现出卓越的性能，与基线相比 (SD) 实现了 95.95% 的准确度和更高的 <code>OA</code>。尽管 <code>OA</code>没有直接评估布局，但改进可以通过非引导 SD 经常无法在非典型组合中生成正确的语义这一事实来解释。我们还注意到，虽然<code>DALLE-v2</code>[36] 总体上实现了最高的<code>OA</code>，但与 SD 相比，布局指令似乎更难，如<code>VISORcond</code>分数较低所示。</p>
<blockquote>
<p>表3：与其他布局到图像模型进行比较。我们的方法提高了空间保真度（由更高的 <code>AP/mAP</code> 分数建议）。<code>mAP</code>以0.3的<code>IoU</code>阈值计算</p>
</blockquote>
<p><img src="/2024/08/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB2%EF%BC%9ATraining-Free-Layout-Control-with-Cross-Attention-Guidance/image_bg5Cyc4_mY.png"></p>
<p>在表 3 中，我们将我们的反向指导与其他布局条件机制进行了比较。除了最后两行的条目外，所有方法都基于Stable Diffusion V1.5[40]。值得注意的是，我们的反向引导大大超过了其他布局条件反射方法，在<code>COCO</code>和<code>Flickr30K</code>上实现了<code>mAP</code>和<code>APP</code>的9点改进。值得注意的是，与并发<code>BoxDiff</code>模型[50]直接比较，我们在<code>MAP</code>和<code>APP</code>中获得了11.6的增益，在保持类似的图像质量的同时，都获得了9.6的增益。最后，我们表明我们的方法可以互补地用于 <code>GLIGEN</code>[27] 等方法，这些方法训练额外的层进行布局调节，进一步提高了它们的性能。</p>
<p><img src="/2024/08/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB2%EF%BC%9ATraining-Free-Layout-Control-with-Cross-Attention-Guidance/image_Pw-lNa5BFz.png"></p>
<blockquote>
<p>图6：不同文本到图像模型与[16]中定义的文本提示的定性比较。如 [16] 中所述，当前的文本到图像模型无法在没有显式布局调节的情况下理解空间关系。然而，我们在交叉注意图的指导下实现了对生成的图像的控制。</p>
</blockquote>
<p>在图 6 中，我们使用从 [16] 采样的提示定性地比较不同的文本到图像模型。不使用布局控制的方法不能纯粹基于文本输入来推断对象之间的空间关系，并且通常无法生成一个或两个对象。我们还观察到，即使是布局条件反射的方法在这种情况下也很困难，尤其是那些采用前向引导范式的方法（<code>eDiff-I</code> [2]、<code>HFG</code>[45]）。在 <code>BoxDiff</code>[50] 的情况下，较低的质量可能是由于忽略了特殊标记和损失函数设计的影响。相比之下，我们的方法（后向引导 SD）可以准确地定位场景中的对象，即使它们很少一起看到，例如“雪板”和“碗”，并且在不损失图像保真度的情况下实现了对提示的最佳依从性。</p>
<p><img src="/2024/08/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB2%EF%BC%9ATraining-Free-Layout-Control-with-Cross-Attention-Guidance/image_kqoX1UB7qt.png"></p>
<blockquote>
<p>图7：我们的方法使用用户指定的边界框控制生成的图像内的对象。左边，火烈鸟的大小和位置根据边界框而变化。在右边，我们展示了控制多个对象的能力。</p>
</blockquote>
<p>我们的方法的更多示例如图 7 所示，展示了对一个或多个对象的大小和位置的精确控制，包括非常规对象类别，例如“火烈鸟”或“皮卡丘”和非典型场景组合。</p>
<h2 id="5-4-进一步的分析和应用"><a href="#5-4-进一步的分析和应用" class="headerlink" title="5.4 进一步的分析和应用"></a>5.4 进一步的分析和应用</h2><h3 id="5-4-1-真实图像布局编辑"><a href="#5-4-1-真实图像布局编辑" class="headerlink" title="5.4.1 真实图像布局编辑"></a>5.4.1 真实图像布局编辑</h3><p><img src="/2024/08/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB2%EF%BC%9ATraining-Free-Layout-Control-with-Cross-Attention-Guidance/image_93JvEwiE_q.png"></p>
<blockquote>
<p>图8：左上角是真实图像输入。虚线上方的图像仅使用文本反转 (TI) [15] 和 <code>Dreambooth</code>[42] 生成。虚线下的图像是由我们的方法在<code>Dreambooth</code>和TI之上生成的。</p>
</blockquote>
<p>我们在图 8 中展示了反向布局指导编辑真实图像的潜力，证实了它在改变“狗”的位置、手势和方向（基于边界框的纵横比）以适应新上下文方面的有效性，而不改变其身份。如图所示，仅通过 <code>Dreambooth/TI</code> 无法实现精确控制对象大小和位置的能力，这突出了我们的方法在与图像编辑和操作相关的广泛应用中的潜力。</p>
<h3 id="5-4-2-交叉注意力层和指导步骤"><a href="#5-4-2-交叉注意力层和指导步骤" class="headerlink" title="5.4.2 交叉注意力层和指导步骤"></a>5.4.2 交叉注意力层和指导步骤</h3><p><img src="/2024/08/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB2%EF%BC%9ATraining-Free-Layout-Control-with-Cross-Attention-Guidance/image_s7Ecq3UP6D.png"></p>
<blockquote>
<p>图 9：单词“cat”在不同时间步（从左到右）的不同层（从上到下）的交叉注意力图。</p>
</blockquote>
<p>我们还研究了实现布局控制所需的层和引导步骤的数量。去噪网络各层的交叉注意图如图9所示。我们观察到（下采样）的第一层没有捕捉到关于对象的太多信息（这里是“猫”）。我们发现仅在架构的中间和上采样块上执行反向指导最有效。该图还说明了对象轮廓通常在扩散过程的早期步骤中生成，在 T &#x3D; 20 之前。根据我们的实验，我们发现 10-20 步通常适合指导。补充中给出了额外的定量分析和示例。</p>
<h3 id="5-4-3-损失比例因子"><a href="#5-4-3-损失比例因子" class="headerlink" title="5.4.3 损失比例因子"></a>5.4.3 损失比例因子</h3><p><img src="/2024/08/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB2%EF%BC%9ATraining-Free-Layout-Control-with-Cross-Attention-Guidance/image_7F_LAShwdv.png"></p>
<blockquote>
<p>图10：反向制导中不同损耗尺度的定性比较。我们从左到右增加损失尺度，保持相同的提示和随机种子。随着规模的增加，对象更紧密地约束在边界框内。然而，对于非常高的尺度，保真度显着下降。</p>
</blockquote>
<p>在图 10 中，我们定性地分析了损失比例因子$  η  $的影响。我们观察到，增加损失权重会导致对生成的图像进行更强的控制，但代价是一些保真度，特别是在更高的尺度上。最佳损失规模设置取决于文本提示的难度。例如，像“熊上方的出水口”这样的非典型提示需要更强的指导才能成功生成这两个对象（没有指导，即$ η &#x3D; 0$，没有生成熊）。这表明布局引导有助于生成器“识别”文本提示中的多个对象。</p>
<hr>
<h1 id="6-Conclusions"><a href="#6-Conclusions" class="headerlink" title="6 Conclusions"></a>6 Conclusions</h1><p>在本文中，我们研究了在没有额外训练或微调的情况下操纵大型预训练文本到图像模型生成的图像的空间布局的潜力。通过我们的探索，我们发现交叉注意图和扩散的初始噪声在确定布局方面起着主导作用，即使是特殊标记的交叉注意图也包含有价值的语义和空间信息。我们识别和分析了大多数先前工作背后的机制：前向指导。此外，基于我们的分析，我们提出了一种新的技术“后向引导”，克服了前向引导的缺点。最后，我们通过将无训练策略扩展到真实图像布局编辑等应用程序来展示我们的无训练策略的通用性。</p>
<p>我们以与其术语兼容的方式使用 <code>Flick30K</code>实体和 <code>MS-COCO</code> 数据集。其中一些图像可能会意外包含人脸或其他个人信息，但我们不使用这些图像或图像区域。有关伦理、数据保护和版权的更多详细信息，请参阅 <a target="_blank" rel="noopener" href="https://www.robots.ox.ac.uk/%20%CC%83vedaldi/research/%20union/ethics.html" title="https://www.robots.ox.ac.uk/ ̃vedaldi/research/ union/ethics.html">https://www.robots.ox.ac.uk/ ̃vedaldi/research/ union/ethics.html</a> 。</p>
<hr>
<h1 id="7-Appendix"><a href="#7-Appendix" class="headerlink" title="7 Appendix"></a>7 Appendix</h1><p>本附录包含以下部分：</p>
<ol>
<li><strong>实施细节</strong>：我们提供了实验设置的更多细节，包括网络架构和噪声调度器。</li>
<li><strong>评估数据集和指标</strong>：我们提供了实验部分使用的数据集和评估指标的详细信息。</li>
<li><strong>消融实验</strong>：进行了详细的定量评估，以了解各种组件和超参数选择的影响。我们研究了引导步骤、特定层损失和后向引导损失比例因子的影响。</li>
<li><strong>初始噪声分析</strong>：我们证明了具有相同初始噪声的不同提示生成的图像具有相似的布局。因此，对初始噪声的良好选择对于指导的成功至关重要。此外，我们定量地证明，在交叉注意上使用定义的损失可以实现最佳的初始噪声选择，提高制导性能。</li>
<li><strong>不同tokens分析</strong>：我们可视化不同提示的交叉注意力图，并为仅使用填充标记控制生成图像布局提供了额外的实验。</li>
<li><strong>更多例子</strong>：我们提供了我们方法的额外示例，包括VISOR[16]协议和真实图像编辑示例下的示例。</li>
</ol>
<h2 id="7-1-实现细节"><a href="#7-1-实现细节" class="headerlink" title="7.1 实现细节"></a>7.1 实现细节</h2><p>我们提供了我们的实验设置的更多细节。</p>
<h3 id="7-1-1-网络架构"><a href="#7-1-1-网络架构" class="headerlink" title="7.1.1 网络架构"></a>7.1.1 网络架构</h3><p>在所有实验中，我们使用稳定扩散 (SD) V-1.5 [39] 作为我们的基础模型，无需任何架构修改。扩散模型在自动编码器的潜在空间中进行训练。具体来说，扩散模型采用相对下采样因子为8的U-Net[41]架构。U-Net的下采样分支有三个顺序交叉注意块。U-Net 的中间部分只有一个交叉注意力块。U-Net 的上采样分支具有三个顺序交叉注意力块。在每个交叉注意力块中，按照以下顺序重复层：ResBlock → Self-Attention → Cross-Attention。下采样分支、中间部分和上采样中的交叉注意力块分别有 2、1 和 3 个这样的重复模式。</p>
<h3 id="7-1-2-噪声调度器"><a href="#7-1-2-噪声调度器" class="headerlink" title="7.1.2 噪声调度器"></a>7.1.2 噪声调度器</h3><p><code>LMSDscheudler</code>在我们所有的实验中都使用 51 个时间步长和 beta 值，从 0.00085 开始并以 0.012 结束，遵循线性调度器。我们还采用无类指导，如 [21] 中所建议的，引导范围为 7.5，与之前的工作 [39] 一致。</p>
<h2 id="7-2-评估数据集和指标"><a href="#7-2-评估数据集和指标" class="headerlink" title="7.2 评估数据集和指标"></a>7.2 评估数据集和指标</h2><h3 id="7-2-1-VISOR"><a href="#7-2-1-VISOR" class="headerlink" title="7.2.1 VISOR"></a>7.2.1 VISOR</h3><p>我们遵循[16]中描述的评估过程来计算VISOR度量，该度量旨在量化文本到图像模型的空间理解能力。该指标侧重于两个对象之间的二维关系，例如左、右、上、下等。我们测量对象精度 (<code>OA</code>)，即生成的图像包含文本提示中指定的两个对象的概率。<code>VISOORuncond</code>是生成具有正确空间关系的对象的概率，<code>VISORcond</code>是生成正确空间关系的条件概率，因为这两个对象都是正确生成的。为了生成用于评估的文本提示，我们使用来自 MS COCO 数据集 [29] 的 80 个对象类别，考虑到每个空间关系的两个对象类别的任何组合，总共有 80 × 79 × 4 &#x3D;25,280 个提示。对于每个提示，我们生成单个图像。作为布局引导输入，我们将图像画布分为垂直或水平两个，根据文本提示定义的空间关系类型创建两个相邻的边界框。这仅对布局施加了弱约束，并且可以自动完成（不需要用户干预）。为了与之前在[16]中评估的方法进行公平的比较，我们在计算VISOR度量时使用与[16]相同的检测模型(OWL-ViT[31])。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://zhouzimu.top">弘树</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://zhouzimu.top/2024/08/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB2%EF%BC%9ATraining-Free-Layout-Control-with-Cross-Attention-Guidance/">http://zhouzimu.top/2024/08/15/论文精读2：Training-Free-Layout-Control-with-Cross-Attention-Guidance/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://zhouzimu.top" target="_blank">诺亚方舟</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/layout-guidance/">layout-guidance</a><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB/">论文精读</a></div><div class="post_share"><div class="social-share" data-image="https://github.com/silent-chen/layout-guidance/raw/gh-page/resources/teaser.png?raw=true" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>码字不易，如果对你有帮助的话请喝一杯奶茶吧~</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.png" target="_blank"><img class="post-qr-code-img" src="/img/wechat.png" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.png" target="_blank"><img class="post-qr-code-img" src="/img/alipay.png" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/08/16/2024%E5%B9%B4%E8%A5%BF%E4%BA%A4%E5%A4%9A%E5%AA%92%E4%BD%93%E5%B0%8F%E7%BB%84%E8%80%83%E6%A0%B8%E4%BB%BB%E5%8A%A11/" title="2024年西交多媒体小组考核任务1"><img class="cover" src="https://p1-bk.byteimg.com/tos-cn-i-mlhdmxsy5m/3f2b1548a7c44c0ca72b469abae9e8e0~tplv-mlhdmxsy5m-q75:0:0.image" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">2024年西交多媒体小组考核任务1</div></div></a></div><div class="next-post pull-right"><a href="/2024/08/14/%E8%AE%A1%E7%BB%84%E9%9D%A2%E7%BB%8F3%EF%BC%9A%E6%8C%87%E4%BB%A4%E7%B3%BB%E7%BB%9F%E5%92%8CCPU/" title="计组面经3：指令系统和CPU"><img class="cover" src="https://beebom.com/wp-content/uploads/2022/09/Intel-Featured-Image.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">计组面经3：指令系统和CPU</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/08/27/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB3%EF%BC%9AGNN/" title="论文精读3：GNN技术博客"><img class="cover" src="https://pic4.zhimg.com/v2-3601304d88287b2b8e1736ded4036a43_1440w.jpg?source=172ae18b" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-27</div><div class="title">论文精读3：GNN技术博客</div></div></a></div><div><a href="/2024/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB4%EF%BC%9AGAN/" title="论文精读4：GAN"><img class="cover" src="https://i0.wp.com/bdtechtalks.com/wp-content/uploads/2018/05/Generative-adversarial-networks-GAN-min-389357362-1527503338782.png?fit=2997%2C1840&ssl=1" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-15</div><div class="title">论文精读4：GAN</div></div></a></div><div><a href="/2024/11/14/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB5%EF%BC%9ADeep-Neural-Networks-for-YouTube-Recommendations/" title="论文精读5：DNN for YouTube Rec"><img class="cover" src="https://blog.acolyer.org/wp-content/uploads/2016/09/dnn-youtube-fig-7.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-11-14</div><div class="title">论文精读5：DNN for YouTube Rec</div></div></a></div><div><a href="/2024/07/27/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB1%EF%BC%9AAn-Attention-Free-Transformer/" title="论文精读1：An Attention Free Transformer"><img class="cover" src="https://th.bing.com/th/id/R.c711422d32a03183ff82c805e6837a5b?rik=hgP3vHF2D%2fyCGA&riu=http%3a%2f%2fwww.deeplearningdaily.com%2fwp-content%2fuploads%2f2021%2f06%2funofficial-pytorch-implementation-of-attention-free-transformer-aft-layers_60c5308a5335f.jpeg&ehk=tQubmdKcevvpCuBVL2VoQQ80Z%2bRnw1ORYVBowyf4VyE%3d&risl=&pid=ImgRaw&r=0" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-27</div><div class="title">论文精读1：An Attention Free Transformer</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="utterances-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/pic_editor_1635545191.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">弘树</div><div class="author-info__description">在我坚定无比的内心，总以最坚强的节奏解开案情</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">123</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">45</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/NoyeArk"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/NoyeArk" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:horiki0@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="https://leetcode.cn/u/horiki/" target="_blank" title="Leetcode"><i class="fas fa-l" style="color: #59e285;"></i></a><a class="social-icon" href="https://www.kaggle.com/horiki" target="_blank" title="Kaggle"><i class="fas fa-k" style="color: #4a7dbe;"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc" style="font-size: 15px;"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Abstract"><span class="toc-text">1 Abstract</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-Introduction"><span class="toc-text">2 Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-Related-Work"><span class="toc-text">3 Related Work</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E6%96%87%E6%9C%AC%E5%88%B0%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90"><span class="toc-text">3.1 文本到图像生成</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E5%9B%BE%E5%83%8F%E7%94%9F%E6%88%90%E4%B8%AD%E7%9A%84%E5%B8%83%E5%B1%80%E6%8E%A7%E5%88%B6"><span class="toc-text">3.2 图像生成中的布局控制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E5%9F%BA%E4%BA%8E%E6%89%A9%E6%95%A3%E7%9A%84%E5%9B%BE%E5%83%8F%E7%BC%96%E8%BE%91"><span class="toc-text">3.3 基于扩散的图像编辑</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-Method"><span class="toc-text">4 Method</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C%EF%BC%9AStable-Diffusion"><span class="toc-text">4.1 准备工作：Stable Diffusion</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E5%B8%83%E5%B1%80%E6%8C%87%E5%AF%BC"><span class="toc-text">4.2 布局指导</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-1-%E5%89%8D%E5%90%91%E6%8C%87%E5%AF%BC"><span class="toc-text">4.2.1 前向指导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-2-%E5%8F%8D%E5%90%91%E6%8C%87%E5%AF%BC"><span class="toc-text">4.2.2 反向指导</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-%E5%88%86%E6%9E%90%E4%B8%8E%E8%AE%A8%E8%AE%BA"><span class="toc-text">4.3 分析与讨论</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-1-Word-Tokens"><span class="toc-text">4.3.1 Word Tokens</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-2-Special-Tokens"><span class="toc-text">4.3.2 Special Tokens</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-3-Initial-Noise"><span class="toc-text">4.3.3 Initial Noise</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-4-Forward-vs-Backward"><span class="toc-text">4.3.4 Forward vs Backward</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-4-%E7%9C%9F%E5%AE%9E%E5%9B%BE%E5%83%8F%E5%B8%83%E5%B1%80%E7%BC%96%E8%BE%91"><span class="toc-text">4.4 真实图像布局编辑</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-Experiments"><span class="toc-text">5 Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-%E5%AE%9E%E9%AA%8C%E7%8E%AF%E5%A2%83"><span class="toc-text">5.1 实验环境</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-1-%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82"><span class="toc-text">5.1.1 实现细节</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-2-%E8%AF%84%E4%BC%B0%E5%9F%BA%E5%87%86"><span class="toc-text">5.1.2 评估基准</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-%E5%89%8D%E5%90%91%E4%B8%8E%E5%90%8E%E5%90%91%E6%8C%87%E5%AF%BC"><span class="toc-text">5.2 前向与后向指导</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-%E4%B8%8E%E5%85%88%E5%89%8D%E5%B7%A5%E4%BD%9C%E7%9A%84%E6%AF%94%E8%BE%83"><span class="toc-text">5.3 与先前工作的比较</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-4-%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%9A%84%E5%88%86%E6%9E%90%E5%92%8C%E5%BA%94%E7%94%A8"><span class="toc-text">5.4 进一步的分析和应用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-1-%E7%9C%9F%E5%AE%9E%E5%9B%BE%E5%83%8F%E5%B8%83%E5%B1%80%E7%BC%96%E8%BE%91"><span class="toc-text">5.4.1 真实图像布局编辑</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-2-%E4%BA%A4%E5%8F%89%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%B1%82%E5%92%8C%E6%8C%87%E5%AF%BC%E6%AD%A5%E9%AA%A4"><span class="toc-text">5.4.2 交叉注意力层和指导步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-3-%E6%8D%9F%E5%A4%B1%E6%AF%94%E4%BE%8B%E5%9B%A0%E5%AD%90"><span class="toc-text">5.4.3 损失比例因子</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-Conclusions"><span class="toc-text">6 Conclusions</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-Appendix"><span class="toc-text">7 Appendix</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#7-1-%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82"><span class="toc-text">7.1 实现细节</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-1-%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84"><span class="toc-text">7.1.1 网络架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-2-%E5%99%AA%E5%A3%B0%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="toc-text">7.1.2 噪声调度器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-2-%E8%AF%84%E4%BC%B0%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%8C%87%E6%A0%87"><span class="toc-text">7.2 评估数据集和指标</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-1-VISOR"><span class="toc-text">7.2.1 VISOR</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline" style="font-size: 17px;"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list" style="font-size: 16px;"><div class="aside-list-item"><a class="thumbnail" href="/2025/05/02/5-%E6%95%B0%E6%8D%AE%E9%9B%86/" title="「HFLLM」5-数据集"><img src="https://cdn.mos.cms.futurecdn.net/gHfBJ6FBHKnLbW36hEDvgV.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「HFLLM」5-数据集"/></a><div class="content"><a class="title" href="/2025/05/02/5-%E6%95%B0%E6%8D%AE%E9%9B%86/" title="「HFLLM」5-数据集">「HFLLM」5-数据集</a><time datetime="2025-05-02T08:26:25.000Z" title="发表于 2025-05-02 16:26:25">2025-05-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/05/01/3-%E5%BE%AE%E8%B0%83%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/" title="「HFLLM」3. 微调预训练模型"><img src="https://venturebeat.com/wp-content/uploads/2023/05/Untitled-design-78.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「HFLLM」3. 微调预训练模型"/></a><div class="content"><a class="title" href="/2025/05/01/3-%E5%BE%AE%E8%B0%83%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/" title="「HFLLM」3. 微调预训练模型">「HFLLM」3. 微调预训练模型</a><time datetime="2025-05-01T02:51:22.000Z" title="发表于 2025-05-01 10:51:22">2025-05-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/17/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB9%EF%BC%9AADSNet/" title="论文精读9：ADSNet"><img src="https://i.ytimg.com/vi/v5BoeTyOAhM/hq720.jpg?sqp=-oaymwE7CK4FEIIDSFryq4qpAy0IARUAAAAAGAElAADIQj0AgKJD8AEB-AH-CYACzgWKAgwIABABGFsgYShlMA8=&amp;rs=AOn4CLChXkjgk_egO-1uGInclI_lQe_MMg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文精读9：ADSNet"/></a><div class="content"><a class="title" href="/2025/02/17/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB9%EF%BC%9AADSNet/" title="论文精读9：ADSNet">论文精读9：ADSNet</a><time datetime="2025-02-17T09:09:52.000Z" title="发表于 2025-02-17 17:09:52">2025-02-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/27/2024%E5%B9%B4AI%E5%B9%B4%E5%BA%A6%E5%85%B3%E9%94%AE%E8%AF%8D/" title="2024年AI年度关键词"><img src="https://image.uisdc.com/wp-content/uploads/2025/01/banner2025012108482456.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2024年AI年度关键词"/></a><div class="content"><a class="title" href="/2025/01/27/2024%E5%B9%B4AI%E5%B9%B4%E5%BA%A6%E5%85%B3%E9%94%AE%E8%AF%8D/" title="2024年AI年度关键词">2024年AI年度关键词</a><time datetime="2025-01-27T11:03:16.000Z" title="发表于 2025-01-27 19:03:16">2025-01-27</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://github.com/silent-chen/layout-guidance/raw/gh-page/resources/teaser.png?raw=true')"><div id="footer-wrap" style="padding: 5px 5px;"><div class="copyright">&copy;2024 - 2025 By 弘树</div><div id="running-time" style="font-size: 14px;"><script>setInterval(() => {
  let create_time = Math.round((new Date(2024, 1, 23).getTime()) / 1000);
  let timestamp = Math.round((new Date().getTime()) / 1000);
  let second = timestamp - create_time;
  let time = new Array(0, 0, 0, 0, 0);
  if (second >= 365 * 24 * 3600) {
      time[0] = parseInt(second / (365 * 24 * 3600));
      second %= 365 * 24 * 3600;
  }
  if (second >= 24 * 3600) {
      time[1] = parseInt(second / (24 * 3600));
      second %= 24 * 3600;
  }
  if (second >= 3600) {
      time[2] = parseInt(second / 3600);
      second %= 3600;
  }
  if (second >= 60) {
      time[3] = parseInt(second / 60);
      second %= 60;
  }
  if (second > 0) {
      time[4] = second;
  }
  currentTimeHtml = "本站已安全运行 " +
      time[0] + " 年 " +
      (time[1] + 31) + " 天 " +
      time[2] + " 时 " +
      time[3] + " 分 " +
      time[4] + " 秒";
  document.getElementById("running-time").innerHTML = currentTimeHtml;
  }, 1000);</script></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const loadUtterances = () => {
    let ele = document.createElement('script')
    ele.id = 'utterances_comment'
    ele.src = 'https://utteranc.es/client.js'
    ele.setAttribute('repo', 'NoyeArk/noyeark.github.io')
    ele.setAttribute('issue-term', 'pathname')
    const nowTheme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'photon-dark' : 'github-light'
    ele.setAttribute('theme', nowTheme)
    ele.crossOrigin = 'anonymous'
    ele.async = true
    document.getElementById('utterances-wrap').appendChild(ele)
  }

  const utterancesTheme = theme => {
    const iframe = document.querySelector('.utterances-frame')
    if (iframe) {
      const theme = theme === 'dark' ? 'photon-dark' : 'github-light'
      const message = {
        type: 'set-theme',
        theme: theme
      };
      iframe.contentWindow.postMessage(message, 'https://utteranc.es');
    }
  }

  btf.addGlobalFn('themeChange', utterancesTheme, 'utterances')

  if ('Utterances' === 'Utterances' || !false) {
    if (false) btf.loadComment(document.getElementById('utterances-wrap'), loadUtterances)
    else loadUtterances()
  } else {
    window.loadOtherComment = loadUtterances
  }
})()</script></div><script src="/js/jquery.js"></script><script src="/js/footer.js"></script><script src="/js/nav.js"></script><script>let tianliGPT_postSelector = '\#post \#article-container';let tianliGPT_key = 'iyiSf8ljbaf';</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div></body></html>