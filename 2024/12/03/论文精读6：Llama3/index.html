<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>论文精读6：Llama3 | 诺亚方舟</title><meta name="author" content="弘树"><meta name="copyright" content="弘树"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Llama3 现代人工智能（AI）系统是由基础模型提供动力的。本文提出了一套新的基础模型，称为Llama 3。它是一群原生支持多语言、代码、推理和工具使用的语言模型。我们最大的模型是一个稠密的Transformer，具有405B参数和高达128K tokens的上下文窗口。本文对Llama 3进行了广泛的实证评价。我们发现，Llama 3在大量任务上提供了与GPT-4等领先语言模型相当的水平。我们">
<meta property="og:type" content="article">
<meta property="og:title" content="论文精读6：Llama3">
<meta property="og:url" content="http://zhouzimu.top/2024/12/03/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB6%EF%BC%9ALlama3/index.html">
<meta property="og:site_name" content="诺亚方舟">
<meta property="og:description" content="Llama3 现代人工智能（AI）系统是由基础模型提供动力的。本文提出了一套新的基础模型，称为Llama 3。它是一群原生支持多语言、代码、推理和工具使用的语言模型。我们最大的模型是一个稠密的Transformer，具有405B参数和高达128K tokens的上下文窗口。本文对Llama 3进行了广泛的实证评价。我们发现，Llama 3在大量任务上提供了与GPT-4等领先语言模型相当的水平。我们">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.analyticsvidhya.com/wp-content/uploads/2024/04/Introducing-Meta-Llama-3-The-most-capable-openly-available-LLM-to-date-scaled.jpg">
<meta property="article:published_time" content="2024-12-03T01:29:32.000Z">
<meta property="article:modified_time" content="2025-01-05T06:59:22.000Z">
<meta property="article:author" content="弘树">
<meta property="article:tag" content="大模型">
<meta property="article:tag" content="Llama3">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.analyticsvidhya.com/wp-content/uploads/2024/04/Introducing-Meta-Llama-3-The-most-capable-openly-available-LLM-to-date-scaled.jpg"><link rel="shortcut icon" href="/img/favicon.jpg"><link rel="canonical" href="http://zhouzimu.top/2024/12/03/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB6%EF%BC%9ALlama3/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":1,"unescape":true,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: 弘树","link":"链接: ","source":"来源: 诺亚方舟","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '论文精读6：Llama3',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-01-05 14:59:22'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/myStyle.css"><link rel="stylesheet" href="/css/tianli_gpt.css"><meta name="generator" content="Hexo 6.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/pic_editor_1635545191.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">123</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">45</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 其他</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-asterisk"></i><span> 归档</span></a></li><li><a class="site-page child" href="/update/"><i class="fa-fw fas fa-archive"></i><span> 更新日志</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url('https://cdn.analyticsvidhya.com/wp-content/uploads/2024/04/Introducing-Meta-Llama-3-The-most-capable-openly-available-LLM-to-date-scaled.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="诺亚方舟"><span class="site-name">诺亚方舟</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 其他</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-asterisk"></i><span> 归档</span></a></li><li><a class="site-page child" href="/update/"><i class="fa-fw fas fa-archive"></i><span> 更新日志</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">论文精读6：Llama3</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2024-12-03T01:29:32.000Z" title="发表于 2024-12-03 09:29:32">2024-12-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">大模型</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">13.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>42分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="论文精读6：Llama3"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Llama3"><a href="#Llama3" class="headerlink" title="Llama3"></a>Llama3</h1><blockquote>
<p>现代人工智能（AI）系统是由基础模型提供动力的。本文提出了一套新的基础模型，称为Llama 3。它是一群原生支持多语言、代码、推理和工具使用的语言模型。我们最大的模型是一个稠密的Transformer，具有<code>405B</code>参数和高达<code>128K</code> tokens的上下文窗口。本文对Llama 3进行了广泛的实证评价。我们发现，Llama 3在大量任务上提供了与<code>GPT-4</code>等领先语言模型相当的水平。我们公开发布了Llama 3，包括预训练和后训练的<code>405B</code>参数语言模型的版本，以及我们的Llama Guard 3模型的输入和输出安全。本文还介绍了我们通过合成方法将图像、视频和语音能力整合到Llama 3中的实验结果。我们观察到，这种方法在图像、视频和语音识别任务上与最先进的方法相竞争。生成的模型还没有被广泛发布，因为它们还在开发中。</p>
</blockquote>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>基础模型是语言、视觉、语音或其他模式的通用模型，它们被设计用来支持大量的人工智能任务。它们构成了许多现代人工智能系统的基础。</p>
<p>现代基础模型的发展包括两个主要阶段：</p>
<ol>
<li><strong>预训练阶段：</strong>模型在大规模训练使用直接任务如单词预测或字幕</li>
<li><strong>后训练阶段：</strong>模型调整遵循指令，符合人类的偏好，提高特定的能力（例如编码和推理）</li>
</ol>
<p>在本文中，我们提出了一套新的语言基础模型，称为Llama 3。Llama 3系列模型原生支持多语言、编码、推理和工具使用。我们最大的模型是具有<code>405B</code>参数的密集Transformer，在高达<code>128K</code>令牌的上下文窗口中处理信息。表1列出了模型群中的每个成员。本文中给出的所有结果都是针对Llama 3.1模型的，为了简洁起见，我们将其称为Llama 3。</p>
<blockquote>
<p>表1：模型<code>Llama3</code>模型群概述。本文中的所有结果都是针对 Llama 3.1模型。</p>
</blockquote>
<p><img src="/2024/12/03/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB6%EF%BC%9ALlama3/image_IEQpvhLCPi.png"></p>
<p>我们相信在开发高质量的基础模型中有三个关键因素：<strong>数据、规模和管理复杂性</strong>。我们试图在开发过程中优化这三个问题：</p>
<ul>
<li><strong>数据</strong>：与之前版本的Llama（Touvron et al.，2023a, b）相比，我们改进了用于预训练和后训练的数据的数量和质量。这些改进包括为训练前数据开发更仔细的预处理和管理管道，以及为训练后数据开发更严格的质量保证和过滤方法。我们在一个大约<code>15T</code>个多语言标记的语料库上对Llama 3进行了预训练，而Llama 2的标记为<code>1.8T</code>。</li>
<li><strong>规模</strong>：我们训练了一个比以前的Llama模型更大的模型：我们的旗舰版语言模型使用$3.8×10^{25}$ FLOPs进行预训练，比最大的版本多了近$50×$。具体来说，我们在<code>15.6T</code>文本标记上预先训练了一个具有40个<code>5B</code>可训练参数的旗舰模型。正如预期的那样基础模型的scaling low，我们的旗舰模型优于使用相同程序训练的较小模型。虽然我们的scaling low表明我们的旗舰模型对于我们的训练预算来说是一个近似计算最优的大小，但我们也训练我们的较小模型比计算最优模型长得多。在相同的推理预算下，得到的模型的性能优于计算最优模型。我们使用旗舰模型来进一步提高训练后这些较小模型的质量。</li>
<li><strong>管理复杂性</strong>：我们做出设计选择，试图最大化我们扩展模型开发过程的能力。例如，我们选择了一个标准的稠密 Transformer 模型架构（2017），并进行了轻微的调整，而不是专家混合模型（2017）以最大化训练稳定性。同样，我们采用了相对简单的基于监督微调 (SFT)、拒绝采样 (RS) 和直接偏好优化（DPO）的后训练过程，而不是更复杂的强化学习算法（Ouyang et al., 2022; Schulman et al., 2017），它们往往不太稳定，更难扩展。</li>
</ul>
<p>我们工作的结果是 Llama 3：三个具有 <code>8B</code>、<code>70B</code>和 <code>405B</code>参数的多语言模型的 herd。我们在跨越广泛语言理解任务的众多基准数据集上评估 Llama 3 的性能。此外，我们执行广泛的人工评估，将 Llama 3 与竞争模型进行比较。</p>
<p><img src="/2024/12/03/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB6%EF%BC%9ALlama3/image_oLPQA8IXeC.png"></p>
<blockquote>
<p>表2：微调 Llama 3 模型在关键基准评估上的性能。该表比较了 Llama 3 的 <code>8B</code>、<code>70B</code>和 <code>405B</code>版本与竞争模型的性能。我们将三个模型大小等价类中的每一个中表现最好的模型加粗。$△$表示使用 5-shot 提示（无 <code>CoT</code>）获得的结果。◁表示没有 <code>CoT</code>获得的结果。$♢$表示使用零样本提示获得的$ 9.9%$结果。</p>
</blockquote>
<p>表2给出了旗舰Llama 3模型在关键基准上的性能概述。我们的实验评估表明，我们的旗舰模型在各种任务上的表现与GPT-4（OpenAI, 2023a）等领先语言模型相当，并且接近于与最先进的匹配。我们的较小模型是最好的，优于具有相似参数数量的替代模型（Bai et al., 2023; Jiang et al., 2023）。Llama 3 也比其前身在有用性和无害性之间提供了更好的平衡（Touvron 等人、2023b）。我们在第 5.4 节中详细分析了 Llama 3 的安全性。</p>
<p>我们在 Llama 3 社区许可证的更新版本下公开发布所有三个 Llama 3 模型；参见 <a target="_blank" rel="noopener" href="https://llama.meta.com/" title="https://llama.meta.com">https://llama.meta.com</a>。这包括我们的 405B 参数语言模型的预训练和训练后版本以及我们的 Llama Guard 模型的新版本 (Inan et al., 2023) 用于输入和输出安全。我们希望旗舰模型的公开发布将推动研究界的创新浪潮，并加速迈向人工智能 (<code>AGI</code>) 发展的途径。</p>
<p>作为 Llama 3 开发过程的一部分，我们还为模型开发了多模态扩展，实现了图像识别、视频识别和语音理解能力。这些模型仍处于积极发展阶段，尚未准备好发布。除了我们的语言建模结果之外，本文还展示了我们对这些多模态模型的初步实验的结果。</p>
<hr>
<h1 id="2-General-Overview"><a href="#2-General-Overview" class="headerlink" title="2 General Overview"></a>2 General Overview</h1><p><img src="/2024/12/03/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB6%EF%BC%9ALlama3/image_IuZhfqrboF.png"></p>
<blockquote>
<p>图1：Llama 3 的整体架构和训练说明。Llama 3是一个 Transformer 语言模型，经过训练可以预测文本序列的下一个标记。有关详细信息，请参阅文本。</p>
</blockquote>
<p>Llama 3 的模型架构如图 1 所示。我们的 Llama 3 语言模型的开发包括两个主要阶段：</p>
<p><strong>语言模型预训练</strong>。我们首先将大型多语言文本语料库转换为离散标记，并在结果数据上预训练大型语言模型（LLM）以执行下一个标记预测。在语言模型预训练阶段，模型学习语言的结构，并从“阅读”的文本中获取关于世界的大量知识。为了有效地做到这一点，预训练是大规模执行的：我们使用 $8K $标记的上下文窗口在$  15.6T  $token上预训练具有$  405B  $参数的模型。这个标准的预训练阶段之后是一个持续的预训练阶段，它将支持的上下文窗口增加到$  128K  $个标记。有关详细信息，请参阅第 3 节。</p>
<blockquote>
<p>预训练阶段的窗口大小先从8K慢慢增大到128K。</p>
</blockquote>
<p><strong>语言模型后训练</strong>。预训练的语言模型对语言有丰富的理解，但它还没有按照我们期望助手的方式遵循指令或行为。我们将模型与多轮人工反馈对齐，每一轮都涉及指令调整数据和直接偏好优化（DPO; Rafailov et al., 2024）上的监督微调（SFT）。在这个后训练第2个阶段，我们还整合了新功能，例如工具使用，并观察到其他领域的显着改进，例如编码和推理。有关详细信息，请参阅第 4 节。最后，在预训练阶段还将安全缓解纳入模型中，其细节在第 5.4 节中描述。</p>
<hr>
<h1 id="3-Pre-Training"><a href="#3-Pre-Training" class="headerlink" title="3 Pre-Training"></a>3 Pre-Training</h1><p>语言模型预训练涉及：</p>
<ol>
<li>大规模训练语料库的管理和过滤（如何构造数据集）</li>
<li>模型架构的开发和相应的比例定律来确定模型大小</li>
<li>大规模高效预训练的技术的发展（如何把模型训练起来）</li>
<li>预训练方法的开发</li>
</ol>
<p>下面我们分别介绍这些组件中的每一个。</p>
<h2 id="3-1-预训练数据"><a href="#3-1-预训练数据" class="headerlink" title="3.1 预训练数据"></a>3.1 预训练数据</h2><p>我们从包含知识的各种数据源创建语言模型预训练数据集，直到 2023 年底。我们在每个数据源上应用了几种数据去重方法和数据清洗来获得高质量的token。我们删除包含大量个人身份信息 (<code>PII</code>) 和具有已知成人内容的信息。</p>
<h3 id="3-1-1-Web数据配置"><a href="#3-1-1-Web数据配置" class="headerlink" title="3.1.1 Web数据配置"></a>3.1.1 Web数据配置</h3><p>我们使用的大部分数据来自网络，我们将在下面描述我们的数据清洗过程。</p>
<p>**<code>PII</code>**<strong>和安全过滤</strong>。在其他缓解措施中，我们实现了旨在从网站中删除数据的过滤器可能包含不安全的内容或大量的 <code>PII</code>，根据各种 Meta 安全标准进行排名的领域，以及已知包含成人内容的领域。</p>
<p><strong>文本提取和清理</strong>。我们处理非截断 Web 文档的原始 HTML 内容以提取高质量的多样化文本。为此，我们构建了一个自定义解析器，它提取 HTML 内容并优化样板去除和内容召回的精度。我们在人工评估中评估解析器的质量，将其与针对类似文章内容优化的流行的第三方 HTML 解析器进行比较，发现它表现良好。我们仔细处理具有数学和代码内容的 HTML 页面以保留该内容的结构。我们维护图像 alt 属性文本，因为数学内容通常表示为预渲染图像，其中数学也在 alt 属性中提供。我们通过实验评估了不同的清理配置。我们发现与普通文本相比，markdown 对主要在 Web 数据上训练的模型的性能有害，因此我们删除所有标记标记。</p>
<p><strong>数据去重</strong>。我们在 URL、文档和行级别应用了几轮重复数据删除：</p>
<ul>
<li>URL 级别的去重。我们在整个数据集中执行 URL 级别的重复数据删除。我们保留每个 URL 对应的页面的最新版本。</li>
<li>文档级别的去重。我们在整个数据集中执行全局 MinHash (Broder, 1997) 重复数据删除以删除近乎重复的文档。</li>
<li>行级别的去重。我们执行类似于 ccNet 的激进行级重复数据删除（Wenzek 等人，2019）。我们删除在 30M 个文档的每个桶中出现超过 6 次的行。尽管我们的手动定性分析表明，行级重复数据删除不仅从导航菜单、cookie 警告等各种网站中删除了左样板，而且还删除了频繁的高质量文本，但我们的实证评估显示出显着的改进。</li>
</ul>
<p><strong>启发式过滤</strong>。我们开发了启发式方法来删除额外的低质量文档、异常值和重复过多的文档。一些启发式示例包括：</p>
<ul>
<li>我们使用重复的 n-gram 覆盖率 (Rae et al., 2021) 来去除由重复内容（例如日志记录或错误消息）组成的行。这些行可能很长且独特，因此无法通过行编过滤。</li>
<li>我们使用“dirty word”计数 (Raffel et al., 2020) 过滤掉域块列表未涵盖的成人网站。</li>
<li>我们使用token的分布 Kullback-Leibler 散度来过滤与训练语料库分布相比包含过多异常值标记的文档。</li>
</ul>
<p><strong>基于模型的质量过滤</strong>。此外，我们尝试将各种基于模型的质量分类器应用于子选择高质量的标记。这些包括使用快速分类器，例如 fasttext (Joulin et al., 2017)，经过训练以识别给定文本是否由 Wikipedia 引用（Touvron et al., 2023a），以及更多计算密集型的基于 Roberta 的分类器 (Liu et al., 2019a) 在 Llama 2 预测上进行训练。为了训练基于 Llama 2 的质量分类器，我们创建了一组干净的 Web 文档，描述了质量要求，并指示 Llama 2 的聊天模型以确定文档是否满足这些要求。我们使用 DistilRoberta (Sanh et al., 2019) 为每个文档生成质量分数，以提高效率。我们通过实验评估了各种质量过滤配置的功效。</p>
<p><strong>代码和数据</strong>。与 DeepSeek-AI 等人类似(2024)，我们构建了特定领域的管道，用于提取代码和数学相关的网页。具体来说，代码和推理分类器都是在 Llama 2 注释的网络数据上训练的 DistilRoberta 模型。与上述一般质量分类器不同，我们对包含数学演绎、STEM 领域的推理以及与自然语言交错的代码的目标网页进行提示调优。由于代码和数学的标记分布与自然语言的标记分布有很大不同，因此这些流程实现了特定领域的 HTML 提取、定制的文本特征和过滤启发式方法。</p>
<p><strong>多语言数据</strong>。与我们对上述英语的处理过程类似，我们实现了过滤器来从可能包含 <code>PII</code>或不安全内容的网站中删除数据。我们的多语言文本处理管道有几个独特的特征：</p>
<ul>
<li>我们使用基于快速文本的语言识别模型将文档分类为 176 种语言。</li>
<li>我们在每种语言的数据中执行文档级和行级重复数据删除。</li>
<li>我们应用特定于语言的启发式方法和基于模型的过滤器来去除低质量的文档。</li>
</ul>
<p>此外，我们使用基于多语言 Llama 2 的分类器对多语言文档进行质量排名，以确保优先考虑高质量的内容。我们通过实验确定预训练中使用的多语言标记的数量，平衡英语和多语言基准的模型性能。</p>
<h3 id="3-1-2-数据混合"><a href="#3-1-2-数据混合" class="headerlink" title="3.1.2 数据混合"></a>3.1.2 数据混合</h3><p>为了获得高质量的语言模型，必须仔细确定预训练数据混合中不同数据源的比例。我们确定这种数据混合的主要工具是知识分类和比例定律实验。</p>
<p><strong>知识分类</strong>。我们开发了一个分类器来对 Web 数据中包含的信息类型进行分类，以更有效地确定数据混合。我们使用该分类器对网络上过度表示的数据类别进行下采样，例如艺术和娱乐。</p>
<p><strong>数据混合的缩放规律</strong>。为了确定最佳数据混合，我们进行了缩放定律实验，在该实验中，我们在数据混合上训练几个小模型，并使用它来预测大模型在该混合上的性能（参见第 3.2.1 节）。我们对不同的数据混合多次重复此过程，以选择新的数据混合候选。随后，我们在这个候选数据混合上训练了一个更大的模型，并在几个关键基准上评估该模型的性能。</p>
<p><strong>数据混合摘要</strong>。我们的最终数据混合包含大约 50% 的通用知识、25% 的数学和推理标记、17% 的代码标记和 8% 的多语言标记。</p>
<h3 id="3-1-3-退火数据"><a href="#3-1-3-退火数据" class="headerlink" title="3.1.3 退火数据"></a>3.1.3 退火数据</h3><p>根据经验，我们发现少量高质量代码和数学数据的退火（参见第 3.4.3 节）可以提高预训练模型在关键基准上的性能。类似于李等人(2024b)，我们使用数据混合执行退火，该混合对选定域中的高质量数据进行上采样。我们不会在我们的退火数据中包括来自常用基准的任何训练集。这使我们能够评估 Llama 3 的真实小样本学习能力和域外泛化。</p>
<p>继 OpenAI (2023a) 之后，我们评估了退火在 GSM8k (Cobbe et al., 2021) 和 MATH (Hendrycks et al., 2021b) 训练集上的功效退火。我们发现退火将预训练的 Llama 3 8B 模型在 GSM8k 和 MATH 验证集上的性能分别提高了 24.0% 和 6.4%。然而，405B 模型的改进可以忽略不计，这表明我们的旗舰模型具有很强的上下文学习和推理能力，并且不需要特定的域内训练样本来获得强大的性能。</p>
<h2 id="3-2-模型架构"><a href="#3-2-模型架构" class="headerlink" title="3.2 模型架构"></a>3.2 模型架构</h2><p>Llama 3 使用标准的dense Transformer 架构 (Vaswani et al., 2017)。在模型架构方面，它与 Llama 和 Llama 2 (Touvron et al., 2023a,b) 没有明显区别；我们的性能提升主要是由数据质量和多样性的提高以及训练规模的增加推动的。</p>
<p>与 Llama 2 相比，我们做了一些小的修改：</p>
<ul>
<li>使用具有 8 个键值头的分组查询注意力（GQA）来提高推理速度并减少解码期间键值对缓存的大小。</li>
<li>使用注意掩码来防止同一序列中不同文档之间的自注意力。我们发现这种变化在标准预训练期间的影响有限，但发现它在对非常长的序列的持续预训练中很重要。</li>
</ul>
<p><img src="/2024/12/03/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB6%EF%BC%9ALlama3/image_wFL-K9w32l.png"></p>
<blockquote>
<p><em>数据可以是好几个文档的内容拼接在一起，但是计算注意力的时候只需要计算与我相关的文档内的注意力即可</em></p>
</blockquote>
<ul>
<li>使用具有 <code>128K</code>个标记的词汇表。我们的tokens词汇表将来自 <code>tiktoken3tokenizer</code>的 <code>100K</code>个tokens与 <code>28K</code>个附加tokens相结合，以更好地支持非英语语言。与 Llama 2 分词器相比，我们的新<code>tokenizer</code>在每个token的 3.17 到 3.94 个字符的英语数据样本上提高了压缩率。这使得模型能够为相同数量的训练计算“阅读”更多的文本。我们还发现，从选择非英语语言中添加 <code>28K</code>个token可以提高压缩比和下游性能，而对英语标记化没有影响。</li>
</ul>
<blockquote>
<p><em>Llama 2应该词表大小应该是32K，所以增加幅度比较大</em></p>
</blockquote>
<ul>
<li>将 <code>RoPE</code>基本频率超参数增加到 500,000，这使我们能够更好地支持更长的上下文；熊等人(2023) 表明这个值对于高达 32,768 的上下文长度是有效的。</li>
</ul>
<p><img src="/2024/12/03/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB6%EF%BC%9ALlama3/image_Zi3sRvLLvz.png"></p>
<blockquote>
<p>表 3：Llama 3 的关键超参数概述。我们展示了 8B、70B 和 405B 语言模型的设置。</p>
</blockquote>
<p>Llama 3 405B 使用具有 126 层的架构、16,384 和 128 个注意力头的token表示维度；有关详细信息，请参见表 3。这导致模型大小根据我们数据上的缩放定律近似计算最优，训练预算为$3.8 × 10^{25}$FLOPs。</p>
<h3 id="3-2-1-Scaling-Laws"><a href="#3-2-1-Scaling-Laws" class="headerlink" title="3.2.1 Scaling Laws"></a>3.2.1 Scaling Laws</h3><p>我们开发了缩放定律 (Hoffmann et al., 2022; Kaplan et al., 2020) 来确定给定预训练计算预算的旗舰模型的最佳模型大小。除了确定最优模型大小外，一个主要的挑战是预测旗舰模型在下游基准任务上的性能，因为有几个问题：</p>
<ol>
<li>现有的标度律通常只预测下一个token预测损失，而不是特定的benchmark性能</li>
<li>Scaling Laws可能有噪声且不可靠，因为它们是基于使用小计算预算进行的预训练运行开发的</li>
</ol>
<p>为了应对这些挑战，我们实现了两阶段方法来开发能够准确预测下游基准性能的scaling laws：</p>
<ol>
<li>我们首先建立了计算最优模型在下游任务和训练<code>FLOPs</code>上的负对数似然之间的相关性</li>
<li>我们利用具有更高计算 FLOP 训练的Scaling Laws模型和旧模型，将下游任务的负对数似然与任务准确性相关联。在这一步中，我们专门利用 Llama 2 系列模型</li>
</ol>
<p>这种方法使我们能够在给定特定数量的计算最佳模型训练 FLOP 的情况下预测下游任务性能。我们使用类似的方法来选择我们的预训练数据混合（参见第 3.4 节）。</p>
<p><strong>Scaling law experiments</strong>。具体来说，我们通过使 用$6 × 10^{18}$FLOPs 和 $10^{22}$个 FLOPs 之间的计算预算的预训练模型来构建我们的缩放定律。在每个计算预算中，我们使用每个计算预算的模型大小子集，预训练大小为 40M 和 16B 参数的模型。在这些训练运行中，我们使用余弦学习率计划、线性预热2,000 个训练步骤。根据模型的大小，峰值学习率设置为$  2 × 10^{−4}  $和$ 4 × 10^{−4}$之间。我们将余弦衰减设置为峰值的 0.1。每一步的权重衰减设置为该步骤中学习率的 0.1 倍。我们对每个计算尺度使用固定的批量大小，范围从 250K 和 4M。</p>
<p><img src="/2024/12/03/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB6%EF%BC%9ALlama3/image_CioTFqjIYh.png"></p>
<blockquote>
<p>图 2：$6 × 10^{18}$和$10^{22}$个 <code>FLOPs</code>之间的缩放定律 <code>IsoFLOPs</code>曲线。损失是保留验证集上的负对数似然。我们使用二阶多项式在每个计算尺度上近似测量值。</p>
</blockquote>
<p>这些实验产生了图2中的<code>IsoFLOPs</code>曲线。这些曲线的损失是在一个单独的验证集上测量的。我们使用二阶多项式拟合测量的损失值，并确定每个抛物线的最小值。我们将抛物线的最小值称为相应预训练计算预算的计算最优模型。</p>
<p>我们使用我们确定这种方法的计算最佳模型来预测特定计算预算的最佳训练token数量。为此，我们假设计算预算 C 和训练token的最佳数量$  N ⋆(C)  $之间存在幂律关系：</p>
<p>$$<br>N^{\star}(C)&#x3D;A C^{\alpha}<br>$$</p>
<p>我们使用图 2 中的数据拟合 A 和 α。我们发现$ (α, A) &#x3D; (0.53, 0.29)$，相应的拟合如图 3 所示。将得到的scaling law外推到$ 3.8 × 10^{25}$FLOPs 表明在 16.55T 令牌上训练 402B 参数模型。</p>
<p><img src="/2024/12/03/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB6%EF%BC%9ALlama3/image_dvvCJcxqJ3.png"></p>
<blockquote>
<p>图 3 已识别的计算最佳模型中的训练令牌数量作为预训练计算预算的函数。我们还包括拟合的标度律预测。计算最优模型对应于图 2 中的抛物线最小值。</p>
</blockquote>
<p>一个重要的观察结果是，随着计算预算的增加，<code>IsoFLOPs</code>曲线在最小值附近变得更平坦。这意味着旗舰模型的性能对模型大小和训练令牌之间的权衡的微小变化相对稳健。基于这一观察，我们最终决定用 <code>405B</code>参数训练一个旗舰模型。</p>
<p><strong>预测下游任务的性能</strong>。我们使用生成的计算最优模型来预测基准数据集上旗舰 Llama 3 模型的性能。首先，我们线性关联基准测试中正确答案的（归一化）负对数似然和训练 FLOP。在此分析中，我们仅使用在上述数据混合上训练多达$10^{22}$个 FLOPs 的标度律模型。接下来，我们使用缩放定律模型和 Llama 2 模型建立了对数似然和准确性之间的 sigmoidal 关系，这些模型使用 Llama 2 数据混合和标记器进行训练。</p>
<p><img src="/2024/12/03/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB6%EF%BC%9ALlama3/image_Kg78PW9ll2.png"></p>
<blockquote>
<p>图 4 ARC Challenge 的标度律预测. 左：ARC Challenge 基准上正确答案的归一化负对数似然作为预训练 FLOPs 的函数。右图：ARC Challenge 基准准确度作为正确答案的归一化负对数似然的函数。该分析使我们能够在预训练开始之前预测 ARC Challenge 基准上的模型性能。有关详细信息，请参阅文本。</p>
</blockquote>
<p>我们在图 4 中展示了该实验在图 4 中的 ARC Challenge 基准上的结果。我们发现这种两步缩放定律预测，它外推了超过四个数量级，非常准确：它只略微低估了旗舰 Llama 3 模型的最终性能。</p>
<h2 id="3-3-基础设施、缩放和效率"><a href="#3-3-基础设施、缩放和效率" class="headerlink" title="3.3 基础设施、缩放和效率"></a>3.3 基础设施、缩放和效率</h2><p>我们描述了我们大规模支持 Llama 3 405B 预训练的硬件和基础设施，并讨论了一些优化，从而提高了训练效率。</p>
<h3 id="3-3-1-训练基础设施"><a href="#3-3-1-训练基础设施" class="headerlink" title="3.3.1 训练基础设施"></a>3.3.1 训练基础设施</h3><p>Llama 1 和 2 模型在 Meta 的 AI Research SuperCluster 上进行了训练（Lee 和 Sengupta，2022 年）。正如我们进一步缩放的那样，Llama 3 的训练迁移到 Meta 的生产集群（Lee et al., 2024）。此设置针对生产级可靠性进行了优化，这在我们扩大培训时是必不可少的。</p>
<p><strong>Computer</strong>：Llama 3 405B 在多达 16K H100 GPU 上训练，每个 GPU 在 700W TDP 上运行，80GB HBM3，使用 Meta 的 Grand Teton AI 服务器平台 (Matt Bowman, 2022)。每个服务器都配备了八个 GPU 和两个 CPU。在服务器内，八个 GPU 通过 <code>NVLink</code>连接。使用 MAST (Choudhury et al., 2024)、Meta 的全局规模训练调度器调度训练作业。</p>
<p><strong>Storage</strong>：Tectonic (Pan et al., 2021)、Meta 的通用分布式文件系统用于构建 Llama 3 预训练的存储结构 (Battey and Gupta, 2024)。它在配备 SSD 的 7,500 个服务器中提供了 240 PB 的存储，并支持 2 TB&#x2F;s 的可持续吞吐量和 7 TB&#x2F;s 的峰值吞吐量。一个主要的挑战是支持高度突发的检查点写入，使存储结构在短时间内饱和。检查点保存每个 GPU 的模型状态，每个 GPU 从 1 MB 到 4 GB，用于恢复和调试。我们的目标是在检查点期间最小化 GPU 暂停时间并增加检查点频率，以减少恢复后丢失的工作量。</p>
<p><strong>Network</strong>：Llama 3 405B 基于 Arista 7800 和 Minipack2 开放计算 Project4 OCP 机架交换机在收敛以太网 (RoCE) 结构上使用 RDMA。Llama 3 系列中的较小模型使用 Nvidia Quantum2 Infiniband 结构进行训练。RoCE 和 Infiniband 集群都利用了 GPU 之间的 400 Gbps 互连。尽管这些集群之间存在潜在的网络技术差异，但我们调整了它们，以便为这些大型训练工作负载提供同等的性能。我们进一步详细阐述了我们的 RoCE 网络，因为我们完全拥有自己的设计。</p>
<ul>
<li><strong>网络拓扑</strong>。我们基于 RoCE 的 AI 集群由三层 Clos 网络连接的 24K GPU5（Lee 等人、2024）。在底层，每个机架托管 16 个 GPU，在两个服务器之间拆分，并通过单个 Minipack2 机架顶部 (ToR) 交换机连接。在中间层，192 个这样的机架由 Cluster Switches 连接，形成 3,072 个 GPU 的吊舱，具有全等分带宽，确保没有超额订阅。在顶层，同一数据中心建筑中的八个这样的pod通过Aggregation Switches连接，形成24K GPU的集群。然而，聚合层的网络连接不能保持完整的二分法带宽，而是超额订阅比为1:7。我们的模型并行方法（见第3.3.2节）和训练作业调度器（Choudhury等人2024）都经过优化，以了解网络拓扑，旨在最小化跨pods的网络通信。</li>
<li><strong>负载平衡</strong>。LLM 训练产生拥堵网络流，这些流很难使用 Equal-Cost Multi-Path (ECMP) 路由等传统方法在所有可用的网络路径上负载平衡。为了应对这一挑战，我们采用了两种技术。首先，我们的集体库在两个 GPU 之间创建了 16 个网络流，而不仅仅是一个，从而减少每个流的流量并为负载平衡提供更多流。其次，我们的 Enhanced-ECMP (E-ECMP) 协议通过对数据包 RoCE 标头中的附加字段进行散列，有效地平衡了这 16 个跨不同网络路径的流。</li>
<li><strong>拥塞控制</strong>。我们在关键部分使用深度缓冲交换机(Gangidi et al., 2024)，以适应集体通信模式引起的瞬态拥塞和缓冲。这种设置有助于限制由慢速服务器引起的持久拥塞和网络反向压力的影响，这在训练中很常见。最后，通过 E-ECMP 更好的负载平衡显着减少了拥塞的机会。通过这些优化，我们成功地运行了 24K GPU 集群，没有传统的拥塞控制方法，例如数据中心量化拥塞通知 (DCQCN)。</li>
</ul>
<h3 id="3-3-2-模型Scaling的并行"><a href="#3-3-2-模型Scaling的并行" class="headerlink" title="3.3.2 模型Scaling的并行"></a>3.3.2 模型Scaling的并行</h3><p>为了扩展我们最大模型的训练，我们使用 4D 并行性——四种不同类型的并行方法的组合——对模型进行分片。这种方法有效地将计算分布在许多 GPU 中，并确保每个 GPU 的模型参数、优化器状态、梯度和激活适合其 HBM。我们对4D并行的实现如图5所示。</p>
<p><img src="/2024/12/03/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB6%EF%BC%9ALlama3/image_Q0XyLUmHLr.png"></p>
<blockquote>
<p>图 5：4D 并行性的说明。GPU按照 [TP, CP, PP, DP] 的顺序分为并行组，其中 DP 代表 FSDP。在此示例中，16 个 GPU 配置为 |TP|&#x3D;2、|CP|&#x3D;2、|PP|&#x3D;2 和 |DP|&#x3D;2 的组大小。&#x20;<br>GPU 在 4D 并行性中的位置表示为向量 [D1, D2, D3, D4]，其中 Di 是第 i 个并行维度的索引。在本例中，GPU0[TP0, CP0, PP0, DP0]和GPU1[TP1, CP0, PP0, DP0]在同一个TP组中，GPU0和GPU2在同一个CP组中，GPU0和GPU4在同一个PP组中，GPU0和GPU8在同一个DP组中。</p>
</blockquote>
<p>它结合了tensor并行、pipeline并行、context并行和data并行。</p>
<p>张量并行将单个权重张量分割成不同设备上的多个块。管道并行逐层将模型垂直划分为阶段，以便不同的设备可以在完整模型管道的不同阶段并行处理。上下文并行性将输入上下文划分为段，减少了非常长的序列长度输入的内存瓶颈。我们使用完全分片的数据并行性 (FSDP; Rajbhandari et al., 2020; Ren et al., 2021; Zhao et al., 2023b)，它在实现数据并行性的同时对模型、优化器和梯度进行分片，该并行性在多个 GPU 上并行处理数据并在每个训练步骤后同步。我们对 Llama 3 个分片优化器状态和梯度使用 FSDP，但对于模型分片，我们在前向计算后不会重新分片，以避免后向传递期间的额外全聚集通信。</p>
<p><strong>GPU利用率</strong>。通过对并行配置、硬件和软件的仔细调优，我们实现了表4所示配置的总体BF16模型FLOPs利用率(MFU;Chowdhery等人(2023))为38-43%。在DP&#x3D;64的8K GPU上，MFU略有下降至41%，而在DP&#x3D;64的8K GPU上，MFU略有下降为43%，这是由于在训练过程中保持每批全局令牌所需的每个DP组的批处理大小较低。</p>
<p><img src="/2024/12/03/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB6%EF%BC%9ALlama3/image_Km0faadOZS.png"></p>
<blockquote>
<p>表 4 Llama 3 405B 预训练的每个阶段的缩放配置和 MFU。有关每种类型的并行性的描述，请参见图 5。</p>
</blockquote>
<p><strong>管道并行改进</strong>。我们遇到了现有实现的几个挑战：</p>
<ol>
<li><strong>批量大小约束</strong>。当前的实现对每个 GPU 的支持批量大小有约束，要求它可以被管道阶段的数量整除。对于图 6 中的示例，管道并行的深度优先调度 (DFS) (Narayanan et al., 2021) 需要 N &#x3D; PP &#x3D; 4，而广度优先调度 (BFS; Lamy-Poiriier (2023)) 需要 N &#x3D; M ，其中 M 是微批次的总数，N 是同一阶段前向和后向的连续微批次的数量。然而，预训练通常需要灵活性来调整批量大小。</li>
<li><strong>内存不平衡</strong>。现有的管道并行实现导致资源消耗不平衡。由于嵌入和预热的微批次，第一阶段消耗更多的内存。</li>
<li><strong>计算不平衡</strong>。在模型的最后一层之后，我们需要计算输出和损失，使这个阶段成为执行延迟瓶颈。</li>
</ol>
<p><img src="/2024/12/03/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB6%EF%BC%9ALlama3/image_AW7-1_xKM-.png"></p>
<blockquote>
<p>图 6 Llama 3 中管道并行性的图示。管道并行在四个管道等级（PP 等级 0 到 3）中划分八个管道阶段（0 到 7），其中等级为 0 的 GPU 运行阶段 0 和 4，P 等级为 1 的 GPU 运行阶段 1 和 5 等。彩色块（0 到 9）表示一系列微批次，其中 M 是微批次的总数，N 是同一阶段前向和后向的连续微批次的数量。我们的关键见解是使 N 可调。</p>
</blockquote>
<p>为了解决这些问题，我们修改了我们的管道调度，如图 6 所示，它允许灵活设置——在这种情况下$ N &#x3D; 5$，它可以在每批中运行任意数量的微批次。这使我们能够运行：</p>
<ol>
<li>当我们大规模设置批量大小限制时，微批次比阶段的数量少</li>
<li>更多的微批次来隐藏点对点通信，在 DFS 和广度优先调度 (BFS) 之间找到最佳通信和内存效率的最佳点</li>
</ol>
<p>为了平衡管道，我们分别从第一阶段和最后阶段减少一个 Transformer 层。这意味着第一阶段的第一个模型块只有嵌入，最后一阶段的最后一个模型块只有输出投影和损失计算。为了减少管道气泡，我们使用一个交错调度(Narayanan et al.， 2021)，在一个管道排名上使用V管道阶段。整体管道气泡比为$\frac{\mathrm{PP}-1}{V * M}$。此外，我们在 PP 中采用了异步点对点通信，这大大加快了训练速度，尤其是在文档掩码引入了额外的计算不平衡的情况下。我们启用TORCH_NCCL_AVOID_RECORD_STREADMS，以减少异步点对点通信的内存使用。最后，为了降低内存成本，基于详细的内存分配分析，我们主动释放未用于未来计算的张量，包括每个管道阶段的输入和输出张量，这不会用于未来的计算。通过这些优化，我们可以在 8K 个标记的序列上预训练 Llama 3，而无需激活检查点。</p>
<p><strong>长序列的上下文并行性</strong>。我们利用上下文并行性 (CP) 在缩放 Llama 3 的上下文长度并在长度高达 128K 的非常长的序列上进行训练时提高内存效率。在 CP 中，我们跨序列维度划分，特别是我们将输入序列划分为$  2 × CP  $块，因此每个 CP 排名接收两个块以获得更好的负载平衡。第$  i  $个 CP 等级接收第 i 个和第$  (2 × CP − 1 − i)  $个块。</p>
<p>与现有的 CP 实现在环形结构中重叠通信和计算（Liu et al., 2023a），我们的 CP 实现采用了一种基于全集合的方法，我们首先收集键 (K) 和值 (V) 张量，然后计算局部查询 (Q) 张量块的注意力输出。尽管全聚集通信延迟暴露在关键路径中，但由于两个主要原因，我们仍然采用这种方法：</p>
<ol>
<li>在基于全聚集的 CP 注意力（例如文档掩码）中支持不同类型的注意掩码更容易和更灵活</li>
<li>由于使用 GQA，暴露的全聚集延迟很小，因为通信 K 和 V 张量远小于 Q 张量（Ainslie 等人，2023）</li>
</ol>
<p>因此，注意力计算的时间复杂度比全聚集 (O(S2) 与 O(S) 大一个数量级，其中 S 表示完整因果掩码中的序列长度），使得全聚集开销可以忽略不计。</p>
<p><strong>网络感知并行配置</strong>。并行维度[TP, CP, PP, DP]的顺序针对网络通信进行了优化。最内层的并行性需要最高的网络带宽和最低的延迟，因此通常被限制在同一个服务器内。最外层的并行性可能分布在多跳网络中，并且应该容忍更高的网络延迟。因此，根据网络带宽和延迟的要求，我们按照 [TP, CP, PP, DP] 的顺序放置并行维度。DP（即 FSDP）是最外层的并行性，因为它可以通过异步预取分片模型权重和减少梯度来容忍更长的网络延迟。以最小的通信开销识别最优并行配置，同时避免GPU内存溢出具有挑战性。我们开发了一个内存消耗估计器和一个性能投影工具，帮助我们探索各种并行配置并投影整体训练性能并有效地识别内存差距。</p>
<p><strong>数值稳定性</strong>。通过比较不同并行设置之间的训练损失，我们修复了影响训练稳定性的几个数值问题。为了确保训练收敛，我们在多个微批次的后向计算期间使用 FP32 梯度累积，并在 FSDP 中跨数据并行工作人员减少 FP32 中的散射梯度。对于中间张量，例如视觉编码器输出，在前向计算中使用多次，后向梯度也在 FP32 中累积。</p>
<h3 id="3-3-3-集群通信"><a href="#3-3-3-集群通信" class="headerlink" title="3.3.3 集群通信"></a>3.3.3 集群通信</h3><p>我们对 Llama 3 的集群通信库基于 Nvidia 的 NCCL 库的分叉，称为 NCCLX。NCCLX 显着提高了 NCCL 的性能，特别是对于更高的延迟网络。回想一下，并行度维度的顺序是 [TP, CP, PP, DP]，其中 DP 对应于 FSDP。最外层的并行维度 PP 和 DP 可以通过多跳网络进行通信，延迟高达数十微秒。原始的 NCCL 集体——FSDP 中的全聚集和减少散射，PP 中的点对点——需要数据分块和阶段数据复制。这种方法会产生几个低效率，包括：</p>
<ol>
<li>需要在网络上交换大量小的控制消息以促进数据传输</li>
<li>额外的内存复制操作</li>
<li>使用额外的 GPU 循环进行通信</li>
</ol>
<p>对于 Llama 3 训练，我们通过调整分块和数据传输来解决这些低效率的子集，以适应我们的网络延迟，这对于大集群来说可能高达数十微秒。我们还允许小的控制消息以更高的优先级遍历我们的网络，特别是避免在深度缓冲核心交换机中阻塞的线头。我们正在进行的未来 Llama 版本的工作涉及对 <code>NCCLX</code>进行更深入的更改，以全面解决上述所有问题。</p>
<h3 id="3-3-4-可靠性和操作挑战"><a href="#3-3-4-可靠性和操作挑战" class="headerlink" title="3.3.4 可靠性和操作挑战"></a>3.3.4 可靠性和操作挑战</h3><p>16K GPU训练的复杂性和潜在故障场景超过了我们操作的更大的CPU集群。此外，训练的同步特性使其不易容错——单个 GPU 故障可能需要重新启动整个作业。尽管存在这些挑战，对于 Llama 3，我们在支持自动集群维护的同时实现了超过 90% 的有效训练时间，例如固件和 Linux 内核升级（Vigraham 和 Leonhardi、2024），这导致每天至少一次训练中断。有效的训练时间衡量了在经过时间的有用训练上花费的时间。</p>
<p>在预训练的 54 天快照期间，我们总共经历了 466 个作业中断。其中，由于固件升级或操作员发起的操作(如配置或数据集更新)的自动化维护操作，47个计划中断。剩下的 419 是意外中断。</p>
<p><img src="/2024/12/03/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB6%EF%BC%9ALlama3/image_mGXYRuq2_p.png"></p>
<blockquote>
<p>表5 Llama 3 405B预训练期间54天内的意外中断根本原因分类。约78%的意外中断归因于已确认或疑似硬件问题。</p>
</blockquote>
<p>如表 5 所示。大约 78% 的意外中断归因于确认的硬件问题，例如 GPU 或主机组件故障，或疑似与硬件相关的问题，例如静默数据损坏和计划外的个人主机维护事件。GPU 问题是最大的类别，占所有意外问题的 58.7%。尽管失败次数很大，但在此期间只需要 3 次重大人工干预，其余的问题由自动化处理。</p>
<p>为了提高有效的训练时间，我们减少了作业启动和检查点时间，并开发了快速诊断和问题解析的工具。我们广泛使用 PyTorch 的内置 NCCL 飞行记录器 (Ansel et al., 2024)，这是一种将集体元数据和堆栈跟踪捕获到环形缓冲区的特征，从而允许我们大规模快速诊断挂起和性能问题，特别是在 NCCLX 方面。使用这个，我们有效地记录每个通信事件以及每个集体操作的持续时间，并在 NCCLX 观察狗或心跳超时上自动转储跟踪数据。我们通过在线配置更改 (Tang et al., 2015) 有选择地启用更多计算密集型的跟踪操作和元数据收集，而无需代码发布或作业重新启动。</p>
<p>由于我们网络中 NVLink 和 RoCE 的混合使用，大规模训练中的错误问题变得复杂。NVLink 上的数据传输通常通过 CUDA 内核发出的负载&#x2F;存储操作发生，远程 GPU 或 NVLink 连接中的故障通常表现为 CUDA 内核内的停滞负载&#x2F;存储操作，而不会返回清晰的错误代码。NCCLX通过与PyTorch紧密协同设计增强了故障检测和定位的速度和准确性，允许PyTorch访问NCCLX的内部状态和跟踪相关信息。虽然无法完全防止由于 NVLink 故障而导致的停顿，但我们的系统监控通信库的状态，并在检测到此类失速时自动超时。此外，NCCLX 跟踪每个 NCCLX 通信的内核和网络活动，并提供失败的 NCCLX 集体内部状态的快照，包括所有等级之间的已完成和挂起数据传输。我们分析这些数据以调试 NCCLX 缩放问题。</p>
<p>有时，硬件问题可能会导致仍然起作用，但缓慢的掉队者很难检测到。即使是单个掉队者也可以减缓数千个其他 GPU，通常表现为功能强大但通信缓慢。我们开发了工具，以优先考虑所选流程组的潜在有问题的通信。通过仅调查少数顶级嫌疑人，我们通常能够有效地识别掉队者。</p>
<p>一个有趣的观察是环境因素对大规模训练性能的影响。对于 Llama 3 405B，我们注意到基于白天的日$ 1 \sim 2%$ 吞吐量变化。这种波动是影响 GPU 动态电压和频率缩放的更高中日温度的结果。</p>
<p>在训练期间，数万个 GPU 可以同时增加或减少功耗，例如，由于所有等待检查点或集体通信才能完成或关闭整个训练作业的 GPU。当这种情况发生时，它会导致数据中心的功耗在几十兆瓦量级的瞬时波动，拉伸电网的极限。这对我们来说是一个持续的挑战，因为我们为未来的 Llama 模型缩放训练。</p>
<h2 id="3-4-训练技巧"><a href="#3-4-训练技巧" class="headerlink" title="3.4 训练技巧"></a>3.4 训练技巧</h2><p>用于预训练 Llama 3 405B 的配方由三个主要阶段组成：</p>
<ol>
<li>初始预训练</li>
<li>长上下文预训练</li>
<li>退火</li>
</ol>
<p>下面分别描述了三个阶段。我们使用类似的方法来预训练 8B 和 70B 模型。</p>
<h3 id="3-4-1-预训练初始化"><a href="#3-4-1-预训练初始化" class="headerlink" title="3.4.1 预训练初始化"></a>3.4.1 预训练初始化</h3><p>我们使用峰值学习率为$ 8 × 10^{−5}$的 <code>AdamW</code>预训练 Llama 3 405B，线性预热 8,000 步，$cos$学习率计划在 1,200,000 步上衰减为$ 8 × 10^{−7}$。我们在训练早期使用较低的$batch_size$来提高训练稳定性，并随后对其进行提高以提高效率。具体来说，我们使用 4M 个tokens的初始批量大小和长度为 4,096 的序列，并在预训练 252M tokens后将这些值加倍到 8,192 个标记的 8M 序列的批量大小。在对 2.87T tokens进行预训练后，我们将$batch_size$设置为 16M。我们发现这种训练配方非常稳定：我们观察到损失峰值很少，并且不需要干预来纠正模型训练分歧。</p>
<p><strong>调整数据混合</strong>。我们在训练期间对预训练数据混合进行了一些调整，以提高特定下游任务的模型性能。特别是，我们增加了预训练过程中非英语数据的百分比，以提高 Llama 3 的多语言性能。我们还对数学数据进行上采样以提高模型的数学推理性能，我们在预训练后期添加了最近的网络数据，以推进模型的知识截止，我们对后来被确定为质量较低的预训练数据的子集进行下采样。</p>
<h3 id="3-4-2-长文本预训练"><a href="#3-4-2-长文本预训练" class="headerlink" title="3.4.2 长文本预训练"></a>3.4.2 长文本预训练</h3><p>在预训练的最后阶段，我们在长文本上进行训练，以支持多达 128K 个token的上下文窗口。我们不会更早地在长序列上进行训练，因为自注意力层中的计算在序列长度上呈二次增长。我们以增量、预训练来增加支持的上下文长度，直到模型成功地适应了增加的上下文长度。我们通过测量：</p>
<ol>
<li>短上下文评估上的模型性能是否完全恢复来评估成功的适应</li>
<li>该模型完美地解决了长度高达该长度的“大海捞针”任务（在一个长文本中插入一些信息，问这个信息是什么）</li>
</ol>
<p>在 Llama 3 405B 预训练中，我们从原始的 8K 上下文窗口开始并在最终的 128K 上下文窗口中结束，逐步增加上下文长度。这个长上下文预训练阶段使用大约 800B 训练tokens。</p>
<h3 id="3-4-3-退火"><a href="#3-4-3-退火" class="headerlink" title="3.4.3 退火"></a>3.4.3 退火</h3><p>在对最终的 <code>40M</code>token进行预训练期间，我们将学习率线性降成 0，保持 128K token的上下文长度。在这个退火阶段，我们还调整了数据混合，以对质量非常高的数据源进行上采样；参见第 3.1.3 节。最后，我们计算退火过程中模型检查点（Polyak (1991) 平均）的平均值以产生最终的预训练模型。</p>
<p><img src="/2024/12/03/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB6%EF%BC%9ALlama3/image_yuSn6C_uWC.png"></p>
<hr>
<h1 id="4-Post-Training"><a href="#4-Post-Training" class="headerlink" title="4 Post-Training"></a>4 Post-Training</h1><p>我们通过在预训练检查点之上应用几轮训练后 6 或将模型与人工反馈对齐来生成对齐的 Llama 3 模型（Ouyang 等人，2022；Rafailova 等人、2024）。每一轮训练后都涉及监督微调 (SFT)，然后是直接偏好优化 (DPO; Rafailov et al., 2024)，通过人工注释收集或综合生成的示例。我们的训练后建模和数据方法分别在第 4.1 节和第 4.2 节中描述。我们进一步详细介绍了自定义数据管理策略，以改进第 4.3 节中的推理、编码、事实性、多语言、工具使用、长上下文和精确指令。</p>
<h2 id="4-1-建模"><a href="#4-1-建模" class="headerlink" title="4.1 建模"></a>4.1 建模</h2><p>我们的后训练策略的主干是一个奖励模型和语言模型。我们首先使用人工注释的偏好数据在预训练的检查点之上训练奖励模型（参见第 4.1.2 节）。然后，我们使用监督微调（SFT；参见第 4.1.3 节）微调预训练的检查点，并进一步将检查点与直接偏好优化对齐（DPO；参见第 4.1.4 节）。</p>
<p><img src="/2024/12/03/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB6%EF%BC%9ALlama3/image_HUrI-qeiJu.png"></p>
<blockquote>
<p>图 7：Llama 3 的整体训练后方法示意图。我们的后训练策略包括拒绝采样、监督微调和直接偏好优化。有关详细信息，请参阅文本。</p>
</blockquote>
<p>这个过程如图 7 所示。除非另有说明，我们的建模过程适用于 Llama 3 405B，为简单起见，我们将 Llama 3 405B 称为 Llama 3。</p>
<h3 id="4-1-1-聊天对话框格式"><a href="#4-1-1-聊天对话框格式" class="headerlink" title="4.1.1 聊天对话框格式"></a>4.1.1 聊天对话框格式</h3><p>为了调整 LLM 以进行人类-AI 交互，我们需要为模型定义一个聊天对话协议来理解人类指令并执行对话任务。与其前身相比，Llama 3 具有工具使用（第 4.3.5 节）等新功能，这些功能可能需要生成多个消息并将它们发送到单个对话轮次中的不同位置（例如，用户、ipython）。为了支持这一点，我们设计了一种新的多消息聊天协议，该协议使用各种特殊的报头和终止令牌。标题标记用于表示对话中每条消息的来源和目的地。类似地，终止标记指示何时是人类和 AI 说话之间的时间。</p>
<h3 id="4-1-2-奖励建模"><a href="#4-1-2-奖励建模" class="headerlink" title="4.1.2 奖励建模"></a>4.1.2 奖励建模</h3><p>我们训练了一个奖励模型 (RM)，涵盖了预训练checkpoint之上的不同功能。训练目标与 Llama 2 相同，只是我们删除了损失中的边距项，因为我们观察到数据缩放后的改进减少。在 Llama 2 之后，我们在过滤掉具有相似响应的样本后使用我们所有的偏好数据来进行奖励建模。除了（选择、拒绝）响应的标准偏好对之外，注释还为一些提示创建了第三个“编辑响应”，其中对中选择的响应被进一步编辑以改进（参见第 4.2.1 节）。因此，每个偏好排名样本有两个或三个响应，排名清晰（编辑 &amp;gt; 选择 &amp;gt; 拒绝）。在训练期间，我们将提示和多个响应连接到一行，响应随机打乱。这是将响应放在单独的行中并计算分数的标准场景的近似值，但在我们的消融中，这种方法提高了训练效率，而不会损失准确性。</p>
<h3 id="4-1-3-监督微调"><a href="#4-1-3-监督微调" class="headerlink" title="4.1.3 监督微调"></a>4.1.3 监督微调</h3><p>然后使用奖励模型对人工注释提示执行拒绝采样，其细节在第 4.2 节中描述。连同这种拒绝采样数据和其他数据源（包括合成数据），我们使用目标标记上的标准交叉熵损失来微调预训练的语言模型（同时屏蔽提示标记上的损失）。有关数据混合的更多详细信息，请参见第 4.2 节。我们将此阶段称为监督微调（SFT；Wei 等人，2022a；Sanh 等人，2022；Wang 等人，2022b），即使许多训练目标是模型生成的。我们最大的模型在 8.5K 到 9K 步的过程中以 10−5 的学习率进行微调。我们发现这些超参数设置在不同的轮次和数据混合中运行良好。</p>
<h3 id="4-1-4-直接偏好优化"><a href="#4-1-4-直接偏好优化" class="headerlink" title="4.1.4 直接偏好优化"></a>4.1.4 直接偏好优化</h3><p>我们进一步用直接偏好优化 (DPO; Rafailov et al., 2024) 训练我们的 <code>SFT</code>模型以进行人类偏好对齐。对于训练，我们主要使用来自先前对齐轮次的最佳性能模型收集的最近一批偏好数据。因此，我们的训练数据更符合在每一轮中优化的策略模型的分布。我们还探索了 PPO (Schulman et al., 2017) 等策略算法，但发现 DPO 对大规模模型需要更少的计算并表现得更好，尤其是在 IFEval (Zhou et al., 2023) 等以下基准测试中。对于Llama 3，我们使用$10^{−5}$的学习率，并将β超参数设置为0.1。此外，我们对DPO应用了以下算法修改:</p>
<ul>
<li><strong>在DPO损失中mask格式token</strong>：我们从损失中选择的响应和拒绝的响应中屏蔽了特殊的格式标记，包括标题和终止标记（在第 4.1.1 节中描述），以稳定 DPO 训练。我们观察到，拥有这些标记有助于损失可能会导致不希望的模型行为，例如尾部重复或突然生成终止标记。我们假设这是由于 DPO 损失的对比性质——选择和拒绝响应中是否存在公共标记会导致相互冲突的学习目标，因为模型需要同时增加和减少这些标记的可能性。</li>
<li><strong>使用 NLL 损失进行正则化</strong>：我们在所选序列上添加了一个额外的负对数似然 (NLL) 损失项，缩放系数为 0.2，类似于 Pang 等人。 (2024)。这有助于通过保持生成所需的格式并防止所选响应的对数概率降低来进一步稳定 DPO 训练（Pang 等人、2024；Pal 等人、2024）。</li>
</ul>
<h3 id="4-1-5-模型平均"><a href="#4-1-5-模型平均" class="headerlink" title="4.1.5 模型平均"></a>4.1.5 模型平均</h3><p>最后，我们对每个 RM、SFT 或 DPO 阶段使用各种版本数据或超参数的实验获得的模型进行平均（Izmailov 等人，2019；Wortsman 等人，2022；Li 等人，2022）。</p>
<h3 id="4-1-6-Iterative-Rounds"><a href="#4-1-6-Iterative-Rounds" class="headerlink" title="4.1.6 Iterative Rounds"></a>4.1.6 Iterative Rounds</h3><p>继 Llama 2 之后，我们在六轮中应用上述方法。在每个周期中，我们收集新的偏好注释和SFT数据，从最新的模型中采样合成数据。</p>
<h2 id="4-2-后训练数据"><a href="#4-2-后训练数据" class="headerlink" title="4.2 后训练数据"></a>4.2 后训练数据</h2><p>训练后数据组合在语言模型的有用性和行为中起着至关重要的作用。在本节中，我们将讨论我们的人工注释程序和偏好数据收集（第 4.2.1 节）、我们的 SFT 数据的组成（第 4.2.2 节）和数据质量控制和清理方法（第 4.2.3 节）。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://zhouzimu.top">弘树</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://zhouzimu.top/2024/12/03/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB6%EF%BC%9ALlama3/">http://zhouzimu.top/2024/12/03/论文精读6：Llama3/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://zhouzimu.top" target="_blank">诺亚方舟</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">大模型</a><a class="post-meta__tags" href="/tags/Llama3/">Llama3</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.analyticsvidhya.com/wp-content/uploads/2024/04/Introducing-Meta-Llama-3-The-most-capable-openly-available-LLM-to-date-scaled.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>码字不易，如果对你有帮助的话请喝一杯奶茶吧~</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.png" target="_blank"><img class="post-qr-code-img" src="/img/wechat.png" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.png" target="_blank"><img class="post-qr-code-img" src="/img/alipay.png" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/12/04/%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%953%EF%BC%9AMiniMax%E6%8A%80%E6%9C%AF%E4%B8%80%E9%9D%A2/" title="面试记录3：MiniMax技术一面"><img class="cover" src="https://imgslim.geekpark.net/uploads/image/file/48/90/48902a437d2c90ce5c0d93e5fc1d6a71.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">面试记录3：MiniMax技术一面</div></div></a></div><div class="next-post pull-right"><a href="/2024/11/27/%E8%AF%BB%E4%B9%A6%E8%AE%B0%E5%BD%955%EF%BC%9A%E7%BD%AA%E4%B8%8E%E7%BD%9A/" title="读书记录5：罪与罚"><img class="cover" src="https://ichef.bbci.co.uk/ace/ws/640/amz/worldservice/live/assets/images/2014/11/28/141128144107_dostoyevsky.jpg.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">读书记录5：罪与罚</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2024/06/03/%E4%B9%A6%E7%94%9F%C2%B7%E6%B5%A6%E8%AF%AD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%981/" title="书生·浦语大模型实战1"><img class="cover" src="https://cdn.api.hksilicon.com/thumb/article/2312136/5?1" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-03</div><div class="title">书生·浦语大模型实战1</div></div></a></div><div><a href="/2024/06/05/%E4%B9%A6%E7%94%9F%C2%B7%E6%B5%A6%E8%AF%AD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%9E%E6%88%982/" title="书生·浦语大模型实战2"><img class="cover" src="https://p4.itc.cn/q_70/images03/20231011/06926d5743584360bdfc926cfbead3dd.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-05</div><div class="title">书生·浦语大模型实战2</div></div></a></div><div><a href="/2024/06/03/%E4%B9%A6%E7%94%9F%C2%B7%E6%B5%A6%E8%AF%AD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B01/" title="书生·浦语大模型笔记1"><img class="cover" src="https://imgcdn.yicai.com/uppics/images/2023/09/1ffc1191218547d9f60da748cab143c6.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-06-03</div><div class="title">书生·浦语大模型笔记1</div></div></a></div><div><a href="/2024/12/12/%E4%BD%BF%E7%94%A8%E5%A4%A7%E6%A8%A1%E5%9E%8BAPI%E7%94%9F%E6%88%90%E6%91%98%E8%A6%81/" title="使用大模型API生成摘要"><img class="cover" src="https://p8.itc.cn/images01/20230901/4a52b1f97b3f466f9ba18bca500f7b5d.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-12-12</div><div class="title">使用大模型API生成摘要</div></div></a></div><div><a href="/2024/07/17/%E7%AC%AC%E4%BA%8C%E6%9C%9FAI%E5%A4%8F%E4%BB%A4%E8%90%A5%E4%BB%BB%E5%8A%A13%EF%BC%9A%E5%AE%9E%E7%8E%B0RAG%E5%BA%94%E7%94%A8/" title="第二期AI夏令营任务3：实现RAG应用"><img class="cover" src="https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Frag-framework.81dc2cdc.png&w=3840&q=75" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-17</div><div class="title">第二期AI夏令营任务3：实现RAG应用</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="utterances-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/pic_editor_1635545191.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">弘树</div><div class="author-info__description">在我坚定无比的内心，总以最坚强的节奏解开案情</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">123</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">45</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/NoyeArk"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/NoyeArk" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:horiki0@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="https://leetcode.cn/u/horiki/" target="_blank" title="Leetcode"><i class="fas fa-l" style="color: #59e285;"></i></a><a class="social-icon" href="https://www.kaggle.com/horiki" target="_blank" title="Kaggle"><i class="fas fa-k" style="color: #4a7dbe;"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc" style="font-size: 15px;"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Llama3"><span class="toc-text">Llama3</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Introduction"><span class="toc-text">1 Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-General-Overview"><span class="toc-text">2 General Overview</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-Pre-Training"><span class="toc-text">3 Pre-Training</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="toc-text">3.1 预训练数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-1-Web%E6%95%B0%E6%8D%AE%E9%85%8D%E7%BD%AE"><span class="toc-text">3.1.1 Web数据配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-2-%E6%95%B0%E6%8D%AE%E6%B7%B7%E5%90%88"><span class="toc-text">3.1.2 数据混合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-3-%E9%80%80%E7%81%AB%E6%95%B0%E6%8D%AE"><span class="toc-text">3.1.3 退火数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="toc-text">3.2 模型架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-1-Scaling-Laws"><span class="toc-text">3.2.1 Scaling Laws</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E3%80%81%E7%BC%A9%E6%94%BE%E5%92%8C%E6%95%88%E7%8E%87"><span class="toc-text">3.3 基础设施、缩放和效率</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-1-%E8%AE%AD%E7%BB%83%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD"><span class="toc-text">3.3.1 训练基础设施</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-2-%E6%A8%A1%E5%9E%8BScaling%E7%9A%84%E5%B9%B6%E8%A1%8C"><span class="toc-text">3.3.2 模型Scaling的并行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-3-%E9%9B%86%E7%BE%A4%E9%80%9A%E4%BF%A1"><span class="toc-text">3.3.3 集群通信</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-4-%E5%8F%AF%E9%9D%A0%E6%80%A7%E5%92%8C%E6%93%8D%E4%BD%9C%E6%8C%91%E6%88%98"><span class="toc-text">3.3.4 可靠性和操作挑战</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7"><span class="toc-text">3.4 训练技巧</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-1-%E9%A2%84%E8%AE%AD%E7%BB%83%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-text">3.4.1 预训练初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-2-%E9%95%BF%E6%96%87%E6%9C%AC%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-text">3.4.2 长文本预训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-3-%E9%80%80%E7%81%AB"><span class="toc-text">3.4.3 退火</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-Post-Training"><span class="toc-text">4 Post-Training</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-%E5%BB%BA%E6%A8%A1"><span class="toc-text">4.1 建模</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-1-%E8%81%8A%E5%A4%A9%E5%AF%B9%E8%AF%9D%E6%A1%86%E6%A0%BC%E5%BC%8F"><span class="toc-text">4.1.1 聊天对话框格式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-2-%E5%A5%96%E5%8A%B1%E5%BB%BA%E6%A8%A1"><span class="toc-text">4.1.2 奖励建模</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-3-%E7%9B%91%E7%9D%A3%E5%BE%AE%E8%B0%83"><span class="toc-text">4.1.3 监督微调</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-4-%E7%9B%B4%E6%8E%A5%E5%81%8F%E5%A5%BD%E4%BC%98%E5%8C%96"><span class="toc-text">4.1.4 直接偏好优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-5-%E6%A8%A1%E5%9E%8B%E5%B9%B3%E5%9D%87"><span class="toc-text">4.1.5 模型平均</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-6-Iterative-Rounds"><span class="toc-text">4.1.6 Iterative Rounds</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E5%90%8E%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="toc-text">4.2 后训练数据</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline" style="font-size: 17px;"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list" style="font-size: 16px;"><div class="aside-list-item"><a class="thumbnail" href="/2025/05/02/5-%E6%95%B0%E6%8D%AE%E9%9B%86/" title="「HFLLM」5-数据集"><img src="https://cdn.mos.cms.futurecdn.net/gHfBJ6FBHKnLbW36hEDvgV.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「HFLLM」5-数据集"/></a><div class="content"><a class="title" href="/2025/05/02/5-%E6%95%B0%E6%8D%AE%E9%9B%86/" title="「HFLLM」5-数据集">「HFLLM」5-数据集</a><time datetime="2025-05-02T08:26:25.000Z" title="发表于 2025-05-02 16:26:25">2025-05-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/05/01/3-%E5%BE%AE%E8%B0%83%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/" title="「HFLLM」3. 微调预训练模型"><img src="https://venturebeat.com/wp-content/uploads/2023/05/Untitled-design-78.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「HFLLM」3. 微调预训练模型"/></a><div class="content"><a class="title" href="/2025/05/01/3-%E5%BE%AE%E8%B0%83%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/" title="「HFLLM」3. 微调预训练模型">「HFLLM」3. 微调预训练模型</a><time datetime="2025-05-01T02:51:22.000Z" title="发表于 2025-05-01 10:51:22">2025-05-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/17/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB9%EF%BC%9AADSNet/" title="论文精读9：ADSNet"><img src="https://i.ytimg.com/vi/v5BoeTyOAhM/hq720.jpg?sqp=-oaymwE7CK4FEIIDSFryq4qpAy0IARUAAAAAGAElAADIQj0AgKJD8AEB-AH-CYACzgWKAgwIABABGFsgYShlMA8=&amp;rs=AOn4CLChXkjgk_egO-1uGInclI_lQe_MMg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文精读9：ADSNet"/></a><div class="content"><a class="title" href="/2025/02/17/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB9%EF%BC%9AADSNet/" title="论文精读9：ADSNet">论文精读9：ADSNet</a><time datetime="2025-02-17T09:09:52.000Z" title="发表于 2025-02-17 17:09:52">2025-02-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/27/2024%E5%B9%B4AI%E5%B9%B4%E5%BA%A6%E5%85%B3%E9%94%AE%E8%AF%8D/" title="2024年AI年度关键词"><img src="https://image.uisdc.com/wp-content/uploads/2025/01/banner2025012108482456.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2024年AI年度关键词"/></a><div class="content"><a class="title" href="/2025/01/27/2024%E5%B9%B4AI%E5%B9%B4%E5%BA%A6%E5%85%B3%E9%94%AE%E8%AF%8D/" title="2024年AI年度关键词">2024年AI年度关键词</a><time datetime="2025-01-27T11:03:16.000Z" title="发表于 2025-01-27 19:03:16">2025-01-27</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://cdn.analyticsvidhya.com/wp-content/uploads/2024/04/Introducing-Meta-Llama-3-The-most-capable-openly-available-LLM-to-date-scaled.jpg')"><div id="footer-wrap" style="padding: 5px 5px;"><div class="copyright">&copy;2024 - 2025 By 弘树</div><div id="running-time" style="font-size: 14px;"><script>setInterval(() => {
  let create_time = Math.round((new Date(2024, 1, 23).getTime()) / 1000);
  let timestamp = Math.round((new Date().getTime()) / 1000);
  let second = timestamp - create_time;
  let time = new Array(0, 0, 0, 0, 0);
  if (second >= 365 * 24 * 3600) {
      time[0] = parseInt(second / (365 * 24 * 3600));
      second %= 365 * 24 * 3600;
  }
  if (second >= 24 * 3600) {
      time[1] = parseInt(second / (24 * 3600));
      second %= 24 * 3600;
  }
  if (second >= 3600) {
      time[2] = parseInt(second / 3600);
      second %= 3600;
  }
  if (second >= 60) {
      time[3] = parseInt(second / 60);
      second %= 60;
  }
  if (second > 0) {
      time[4] = second;
  }
  currentTimeHtml = "本站已安全运行 " +
      time[0] + " 年 " +
      (time[1] + 31) + " 天 " +
      time[2] + " 时 " +
      time[3] + " 分 " +
      time[4] + " 秒";
  document.getElementById("running-time").innerHTML = currentTimeHtml;
  }, 1000);</script></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const loadUtterances = () => {
    let ele = document.createElement('script')
    ele.id = 'utterances_comment'
    ele.src = 'https://utteranc.es/client.js'
    ele.setAttribute('repo', 'NoyeArk/noyeark.github.io')
    ele.setAttribute('issue-term', 'pathname')
    const nowTheme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'photon-dark' : 'github-light'
    ele.setAttribute('theme', nowTheme)
    ele.crossOrigin = 'anonymous'
    ele.async = true
    document.getElementById('utterances-wrap').appendChild(ele)
  }

  const utterancesTheme = theme => {
    const iframe = document.querySelector('.utterances-frame')
    if (iframe) {
      const theme = theme === 'dark' ? 'photon-dark' : 'github-light'
      const message = {
        type: 'set-theme',
        theme: theme
      };
      iframe.contentWindow.postMessage(message, 'https://utteranc.es');
    }
  }

  btf.addGlobalFn('themeChange', utterancesTheme, 'utterances')

  if ('Utterances' === 'Utterances' || !false) {
    if (false) btf.loadComment(document.getElementById('utterances-wrap'), loadUtterances)
    else loadUtterances()
  } else {
    window.loadOtherComment = loadUtterances
  }
})()</script></div><script src="/js/jquery.js"></script><script src="/js/footer.js"></script><script src="/js/nav.js"></script><script>let tianliGPT_postSelector = '\#post \#article-container';let tianliGPT_key = 'iyiSf8ljbaf';</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div></body></html>