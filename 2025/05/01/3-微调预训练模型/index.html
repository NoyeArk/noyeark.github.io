<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>「HFLLM」3. 微调预训练模型 | 诺亚方舟</title><meta name="author" content="弘树"><meta name="copyright" content="弘树"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="1 介绍本章将学习：  如何从 Hub 准备大型数据集 如何使用高级TrainerAPI 微调模型 如何使用自定义训练循环 如何利用 🤗 Accelerate 库在任何分布式设置上轻松运行自定义训练循环   2 处理数据以下是如何在 PyTorch 中的一个批次上训练序列分类器： 123456789101112131415161718192021import torchfrom torch.op">
<meta property="og:type" content="article">
<meta property="og:title" content="「HFLLM」3. 微调预训练模型">
<meta property="og:url" content="http://zhouzimu.top/2025/05/01/3-%E5%BE%AE%E8%B0%83%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="诺亚方舟">
<meta property="og:description" content="1 介绍本章将学习：  如何从 Hub 准备大型数据集 如何使用高级TrainerAPI 微调模型 如何使用自定义训练循环 如何利用 🤗 Accelerate 库在任何分布式设置上轻松运行自定义训练循环   2 处理数据以下是如何在 PyTorch 中的一个批次上训练序列分类器： 123456789101112131415161718192021import torchfrom torch.op">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://venturebeat.com/wp-content/uploads/2023/05/Untitled-design-78.png">
<meta property="article:published_time" content="2025-05-01T02:51:22.000Z">
<meta property="article:modified_time" content="2025-05-04T08:26:26.327Z">
<meta property="article:author" content="弘树">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://venturebeat.com/wp-content/uploads/2023/05/Untitled-design-78.png"><link rel="shortcut icon" href="/img/favicon.jpg"><link rel="canonical" href="http://zhouzimu.top/2025/05/01/3-%E5%BE%AE%E8%B0%83%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":1,"unescape":true,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: 弘树","link":"链接: ","source":"来源: 诺亚方舟","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '「HFLLM」3. 微调预训练模型',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-05-04 16:26:26'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/myStyle.css"><link rel="stylesheet" href="/css/tianli_gpt.css"><meta name="generator" content="Hexo 6.3.0"><link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="loading-box"><div class="wizard-scene"><div class="wizard-objects"><div class="wizard-square"></div><div class="wizard-circle"></div><div class="wizard-triangle"></div></div><div class="wizard"><div class="wizard-body"></div><div class="wizard-right-arm"><div class="wizard-right-hand"></div></div><div class="wizard-left-arm"><div class="wizard-left-hand"></div></div><div class="wizard-head"><div class="wizard-beard"></div><div class="wizard-face"><div class="wizard-adds"></div></div><div class="wizard-hat"><div class="wizard-hat-of-the-hat"></div><div class="wizard-four-point-star --first"></div><div class="wizard-four-point-star --second"></div><div class="wizard-four-point-star --third"></div></div></div></div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/pic_editor_1635545191.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">123</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">45</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 其他</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-asterisk"></i><span> 归档</span></a></li><li><a class="site-page child" href="/update/"><i class="fa-fw fas fa-archive"></i><span> 更新日志</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg fixed" id="page-header" style="background-image: url('https://venturebeat.com/wp-content/uploads/2023/05/Untitled-design-78.png')"><nav id="nav"><span id="blog-info"><a href="/" title="诺亚方舟"><span class="site-name">诺亚方舟</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 其他</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-asterisk"></i><span> 归档</span></a></li><li><a class="site-page child" href="/update/"><i class="fa-fw fas fa-archive"></i><span> 更新日志</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">「HFLLM」3. 微调预训练模型</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">发表于</span><time datetime="2025-05-01T02:51:22.000Z" title="发表于 2025-05-01 10:51:22">2025-05-01</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/">大模型</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>19分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="「HFLLM」3. 微调预训练模型"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h1><p>本章将学习：</p>
<ol>
<li>如何从 Hub 准备大型数据集</li>
<li>如何使用高级<code>Trainer</code>API 微调模型</li>
<li>如何使用自定义训练循环</li>
<li>如何利用 🤗 Accelerate 库在任何分布式设置上轻松运行自定义训练循环</li>
</ol>
<hr>
<h1 id="2-处理数据"><a href="#2-处理数据" class="headerlink" title="2 处理数据"></a>2 处理数据</h1><p>以下是如何在 PyTorch 中的一个批次上训练序列分类器：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> AdamW</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification</span><br><span class="line"></span><br><span class="line"><span class="comment"># Same as before</span></span><br><span class="line">checkpoint = <span class="string">&quot;bert-base-uncased&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(checkpoint)</span><br><span class="line">sequences = [</span><br><span class="line">    <span class="string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;This course is amazing!&quot;</span>,</span><br><span class="line">]</span><br><span class="line">batch = tokenizer(sequences, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is new</span></span><br><span class="line">batch[<span class="string">&quot;labels&quot;</span>] = torch.tensor([<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">optimizer = AdamW(model.parameters())</span><br><span class="line">loss = model(**batch).loss</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>


<p>当然，仅仅用两句话训练模型不会产生非常好的结果。为了获得更好的结果，您需要准备更大的数据集。</p>
<p>在本节中，我们将使用 William B. Dolan 和 Chris Brockett 在<a target="_blank" rel="noopener" href="https://www.aclweb.org/anthology/I05-5002.pdf" title="一篇论文">一篇论文</a>中介绍的 MRPC （Microsoft Research Paraphrase Corpus） 数据集作为示例。该数据集由 5,801 对句子组成，带有一个标签，指示它们是否是释义（即，如果两个句子的含义相同）。我们在本章中选择它，因为它是一个小数据集，因此很容易对其进行训练。</p>
<h2 id="2-1-从Hub加载数据集"><a href="#2-1-从Hub加载数据集" class="headerlink" title="2.1 从Hub加载数据集"></a>2.1 从Hub加载数据集</h2><p>Hub 不仅包含模型；它还拥有许多不同语言的多个数据集。您可以<a target="_blank" rel="noopener" href="https://huggingface.co/datasets" title="在此处">在此处</a>浏览数据集，我们建议您在完成本节后尝试加载和处理新数据集（请参阅<a target="_blank" rel="noopener" href="https://huggingface.co/docs/datasets/loading" title="此处">此处</a>的一般文档）。但现在，让我们专注于 MRPC 数据集！这是构成 GLUE 基准测试的 10 个数据集之一，<a target="_blank" rel="noopener" href="https://gluebenchmark.com/" title="GLUE 基准测试">GLUE 基准测试</a>是一项学术基准测试，用于衡量 ML 模型在 10 个不同文本分类任务中的性能。</p>
<p>🤗 数据集库提供了一个非常简单的命令，用于在 Hub 上下载和缓存数据集。我们可以像这样下载 MRPC 数据集：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">raw_datasets = load_dataset(<span class="string">&quot;glue&quot;</span>, <span class="string">&quot;mrpc&quot;</span>)</span><br><span class="line">raw_datasets</span><br></pre></td></tr></table></figure>


<p>这里在colab中进行测试，运行结果如下：</p>
<p><img src="/2025/05/01/3-%E5%BE%AE%E8%B0%83%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/image_FhPlOGL6Xo.png"></p>
<p>如你所见，我们得到了一个<code>DatasetDict</code>对象，其中包含训练集、验证集和测试集。每个都包含几列（<code>sentence1</code>、<code>sentence2</code>、<code>label</code>和<code>idx</code>）和可变数量的行，即每个集中的元素数（因此，训练集中有 3,668 对句子，验证集中有 408 对，测试集中有 1,725 对）。</p>
<p>此命令默认在*~&#x2F;.cache&#x2F;huggingface&#x2F;datasets*中下载并缓存数据集。回想一下第 2 章，您可以通过设置<code>HF_HOME</code>环境变量来自定义缓存文件夹。</p>
<p>我们可以通过索引来访问<code>raw_datasets</code>对象中的每对句子，就像使用字典一样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">raw_train_dataset = raw_datasets[<span class="string">&quot;train&quot;</span>]</span><br><span class="line">raw_train_dataset[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>


<p>结果如下：</p>
<p><img src="/2025/05/01/3-%E5%BE%AE%E8%B0%83%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/image_qKsDCKZurG.png"></p>
<p>我们可以看到标签已经是整数，因此我们不必在那里进行任何预处理。要知道哪个整数对应哪个标签，我们可以检查<code>raw_train_dataset</code>的特征。这将告诉我们每列的类型：</p>
<p><img src="/2025/05/01/3-%E5%BE%AE%E8%B0%83%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/image_yD3PNkpXGD.png"></p>
<p>在后台，<code>label</code>的类型为<code>ClassLabel</code>，整数到标签 name 的映射存储在<em>names</em>文件夹中。<code>0</code>对应于<code>not_equivalent，1</code>对应于<code>等效</code>。</p>
<h2 id="2-2-预处理数据集"><a href="#2-2-预处理数据集" class="headerlink" title="2.2 预处理数据集"></a>2.2 预处理数据集</h2><p>要预处理数据集，我们需要将文本转换为模型可以理解的数字。正如您在<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter2" title="上一章">上一章</a>中看到的，这是使用 tokenizer 完成的。我们可以给分词器一个句子或一个句子列表，这样我们就可以直接对每对的所有第一句和所有第二句进行分词，如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer</span><br><span class="line"></span><br><span class="line">checkpoint = <span class="string">&quot;bert-base-uncased&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line">tokenized_sentences_1 = tokenizer(raw_datasets[<span class="string">&quot;train&quot;</span>][<span class="string">&quot;sentence1&quot;</span>])</span><br><span class="line">tokenized_sentences_2 = tokenizer(raw_datasets[<span class="string">&quot;train&quot;</span>][<span class="string">&quot;sentence2&quot;</span>])</span><br></pre></td></tr></table></figure>


<p>但是，我们不能只将两个序列传递给模型并预测这两个句子是否是释义。我们需要将这两个序列作为一对处理，并应用适当的预处理。幸运的是，分词器还可以采用一对序列，并按照我们的 BERT 模型期望的方式进行准备：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inputs = tokenizer(<span class="string">&quot;This is the first sentence.&quot;</span>, <span class="string">&quot;This is the second one.&quot;</span>)</span><br><span class="line">inputs</span><br></pre></td></tr></table></figure>


<p><img src="/2025/05/01/3-%E5%BE%AE%E8%B0%83%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/image_BoQ8maEDOt.png"></p>
<p>之前讨论了<code>input_ids</code>键和<code>attention_mask</code>键，但我们推迟了讨论<code>token_type_ids</code>。在此示例中，这是告诉模型输入的哪一部分是第一句话，哪一部分是第二句话。</p>
<p>如果我们将<code>input_ids</code>中的 ID 解码回单词：</p>
<p><img src="/2025/05/01/3-%E5%BE%AE%E8%B0%83%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/image_KQEj_HJAfs.png"></p>
<p>所以我们看到模型期望输入是当有两个句子<code>[CLS] sentence1 [SEP] sentence2 [SEP]</code>时的形式。将此与<code>token_type_ids</code>保持一致，我们可以得到：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">&#x27;[CLS]&#x27;</span>, <span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;first&#x27;</span>, <span class="string">&#x27;sentence&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;[SEP]&#x27;</span>, <span class="string">&#x27;this&#x27;</span>, <span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;the&#x27;</span>, <span class="string">&#x27;second&#x27;</span>, <span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;.&#x27;</span>, <span class="string">&#x27;[SEP]&#x27;</span>]</span><br><span class="line">[      <span class="number">0</span>,      <span class="number">0</span>,    <span class="number">0</span>,     <span class="number">0</span>,       <span class="number">0</span>,          <span class="number">0</span>,   <span class="number">0</span>,       <span class="number">0</span>,      <span class="number">1</span>,    <span class="number">1</span>,     <span class="number">1</span>,        <span class="number">1</span>,     <span class="number">1</span>,   <span class="number">1</span>,       <span class="number">1</span>]</span><br></pre></td></tr></table></figure>


<p>如您所见，对应于<code>[CLS] sentence1 [SEP]</code>的输入部分的标记类型 ID 均为<code>0</code>，而对应于句子<code>2 [SEP]</code>的其他部分的标记类型 ID 均为<code>1</code>。</p>
<p>请注意，如果您选择其他检查点，则标记化输入中不一定包含<code>token_type_ids</code>（例如，如果您使用 DistilBERT 模型，则不会返回它们）。只有当模型知道如何处理它们时，才会返回它们，因为它在预训练期间已经看到了它们。</p>
<p>在这里，BERT 使用令牌类型 ID 进行预训练，除了我们在<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter1" title="第 1 章">第 1 章</a>中讨论的掩码语言建模目标之外，它还有一个额外的目标，称为<em>下一句预测</em>。此任务的目标是对句子对之间的关系进行建模。</p>
<p>通过下一个句子预测，模型将获得成对的句子（带有随机掩码的标记），并要求预测第二个句子是否在第一个句子之后。为了使任务非同寻常，一半时间句子在提取它们的原始文档中彼此跟随，另一半时间这两个句子来自两个不同的文档。</p>
<p>一般来说，你不需要担心你的分词输入中是否有<code>token_type_ids</code>：只要你对分词器和模型使用相同的检查点，一切都会好起来的，因为分词器知道要为其模型提供什么。</p>
<p>现在我们已经了解了分词器如何处理一对句子，我们可以使用它来分词我们的整个数据集：就像<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter2" title="上一章">上一章</a>一样，我们可以通过给分词器第一个句子的列表，然后是第二个句子的列表，给分词器一个句子对的列表。这也与我们在<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter2" title="Chapter 2">Chapter 2</a>中看到的 padding 和 truncation 选项兼容。因此，预处理训练数据集的一种方法是：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tokenized_dataset = tokenizer(</span><br><span class="line">    raw_datasets[<span class="string">&quot;train&quot;</span>][<span class="string">&quot;sentence1&quot;</span>],</span><br><span class="line">    raw_datasets[<span class="string">&quot;train&quot;</span>][<span class="string">&quot;sentence2&quot;</span>],</span><br><span class="line">    padding=<span class="literal">True</span>,</span><br><span class="line">    truncation=<span class="literal">True</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>


<p>这很好用，但它的缺点是返回字典（包含我们的键、<code>input_ids</code>、<code>attention_mask</code>和<code>token_type_ids</code>，以及列表列表中的值）。在分词化过程中，如果您有足够的 RAM 来存储整个数据集，它也只能起作用（而 Datasets 库中的🤗数据集是存储在磁盘上的<a target="_blank" rel="noopener" href="https://arrow.apache.org/" title="Apache Arrow">Apache Arrow</a>文件，因此您只将请求的样本加载到内存中）。</p>
<p>为了将数据保存为数据集，我们将使用<a target="_blank" rel="noopener" href="https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.map" title="Dataset.map()">Dataset.map()</a>方法。如果我们需要完成更多的预处理而不仅仅是 tokenization，这也为我们提供了一些额外的灵活性。<code>map()</code>方法的工作原理是在数据集的每个元素上应用一个函数，因此让我们定义一个函数来标记我们的输入：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize_function</span>(<span class="params">example</span>):</span><br><span class="line">    <span class="keyword">return</span> tokenizer(example[<span class="string">&quot;sentence1&quot;</span>], example[<span class="string">&quot;sentence2&quot;</span>], truncation=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>


<p>此函数采用一个字典（就像我们数据集中的项目）并返回一个键为<code>input_ids</code>、<code>attention_mask</code>和<code>token_type_ids</code>的新字典。请注意，如果<code>示例</code>词典包含多个样本（每个键都是一个句子列表），它也有效，因为<code>分词器</code>适用于句子对列表，如前所述。这将允许我们在调用<code>map()</code>时使用选项<code>batched=True</code>，这将大大加快分词速度。<code>分词器</code>由 Tokenizers 库中用 Rust 编写的<a target="_blank" rel="noopener" href="https://github.com/huggingface/tokenizers" title="🤗">🤗</a>分词器提供支持。这个分词器可以非常快，但前提是我们一次给它很多输入。</p>
<p>请注意，我们现在在 tokenization 函数中省略了<code>padding</code>参数。这是因为将所有样本填充到最大长度效率不高：最好在构建批次时填充样本，因为这样我们只需要填充到该批次中的最大长度，而不是整个数据集中的最大长度。当输入的长度非常可变时，这可以节省大量时间和处理能力！</p>
<p>以下是我们如何一次在所有数据集上应用分词函数。我们在对<code>map</code>的调用中使用了<code>batched=True</code>，因此该函数一次应用于数据集的多个元素，而不是单独应用于每个元素。这允许更快的预处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tokenized_datasets = raw_datasets.<span class="built_in">map</span>(tokenize_function, batched=<span class="literal">True</span>)</span><br><span class="line">tokenized_datasets</span><br></pre></td></tr></table></figure>


<p>运行结果如下：</p>
<p><img src="/2025/05/01/3-%E5%BE%AE%E8%B0%83%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/image_1fnpaz5YOo.png"></p>
<p>Datasets 库应用此处理的方式🤗是向数据集添加新字段，每个字段对应预处理函数返回的字典中的每个键：</p>
<p>你甚至可以在使用<code>map()</code>应用预处理函数时通过传递<code>num_proc</code>参数来使用 multiprocessing。我们在这里没有这样做，🤗因为 Tokenizers 库已经使用多个线程来更快地对样本进行分词，但如果您没有使用由此库支持的快速分词器，这可能会加快您的预处理速度。</p>
<p>我们的<code>tokenize_function</code>返回一个键为<code>input_ids</code>、<code>attention_mask</code>和<code>token_type_ids</code>的字典，因此这三个字段将添加到数据集的所有分片中。请注意，如果我们的预处理函数为我们应用<code>map()</code>的数据集中的现有 key 返回新值，我们也可以更改现有字段。</p>
<p>我们需要做的最后一件事是在将元素批处理在一起时将所有示例填充到最长元素的长度——这种技术我们称之为<em>动态填充</em>。</p>
<h2 id="2-3-动态填充"><a href="#2-3-动态填充" class="headerlink" title="2.3 动态填充"></a>2.3 动态填充</h2><p>负责将样本放在一个批次中的函数称为<em>collate 函数</em>。这是您可以在构建<code>DataLoader</code>时传递的参数，默认是一个函数，该函数只会将样本转换为 PyTorch 张量并连接它们（如果您的元素是列表、元组或字典，则递归）。在我们的例子中，这是不可能的，因为我们的输入不会都具有相同的大小。我们故意推迟了填充，仅在每个批次上根据需要应用它，并避免出现过多填充的过长输入。这将大大加快训练速度，但请注意，如果您在 TPU 上训练，它可能会导致问题——TPU 更喜欢固定的形状，即使这需要额外的填充。</p>
<p>为了在实践中做到这一点，我们必须定义一个 collate 函数，该函数将对我们想要一起批处理的数据集项目应用正确数量的填充。幸运的是，🤗 Transformers 库通过<code>DßataCollatorWithPadding</code>为我们提供了这样的函数。当你实例化它时，它需要一个分词器（以了解要使用哪个 padding token，以及模型期望 padding 在输入的左侧还是右侧），并且会做你需要的一切：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> DataCollatorWithPadding</span><br><span class="line">data_collator = DataCollatorWithPadding(tokenizer=tokenizer)</span><br></pre></td></tr></table></figure>


<p>为了测试这个新方法，让我们从训练集中获取一些样本，我们想一起批处理这些样本。在这里，我们删除了<code>idx</code>、<code>sentence1</code>和<code>sentence2</code>列，因为它们不需要并且包含字符串（而且我们不能用字符串创建张量），并查看批处理中每个条目的长度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">samples = tokenized_datasets[<span class="string">&quot;train&quot;</span>][:<span class="number">8</span>]</span><br><span class="line">samples = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> samples.items() <span class="keyword">if</span> k <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&quot;idx&quot;</span>, <span class="string">&quot;sentence1&quot;</span>, <span class="string">&quot;sentence2&quot;</span>]&#125;</span><br><span class="line">[<span class="built_in">len</span>(x) <span class="keyword">for</span> x <span class="keyword">in</span> samples[<span class="string">&quot;input_ids&quot;</span>]]</span><br></pre></td></tr></table></figure>


<p>结果如下：</p>
<p><img src="/2025/05/01/3-%E5%BE%AE%E8%B0%83%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/image_5qzpO1FXTV.png"></p>
<p>毫不奇怪，我们得到的样品长度不一，从 32 到 67 不等。动态填充意味着此批次中的样本都应填充到 67 的长度，即该批次内的最大长度。如果没有动态填充，则必须将所有样本填充到整个数据集中的最大长度，或模型可以接受的最大长度。让我们仔细检查一下我们的<code>data_collator</code>是否正确地动态填充了 batch：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch = data_collator(samples)</span><br><span class="line">&#123;k: v.shape <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br></pre></td></tr></table></figure>


<p>看起来不错！现在我们已经从原始文本变成了我们的模型可以处理的批处理，我们准备对其进行微调！</p>
<hr>
<h1 id="3-使用Trainer-API微调模型"><a href="#3-使用Trainer-API微调模型" class="headerlink" title="3 使用Trainer API微调模型"></a>3 使用Trainer API微调模型</h1><p>🤗 Transformers 提供了一个<code>Trainer</code>类，可帮助您微调它在数据集上提供的任何预训练模型。完成上一节中的所有数据预处理工作后，您只剩下几个步骤来定义<code>Trainer</code>。最困难的部分可能是准备环境来运行<code>Trainer.train()，</code>因为它在 CPU 上运行得非常慢。如果您没有设置 GPU，您可以在<a target="_blank" rel="noopener" href="https://colab.research.google.com/" title="Google Colab">Google Colab</a>上访问免费的 GPU 或 TPU。</p>
<p>下面的代码示例假定您已经执行了上一节中的示例。以下是您需要的简短摘要：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, DataCollatorWithPadding</span><br><span class="line"></span><br><span class="line">raw_datasets = load_dataset(<span class="string">&quot;glue&quot;</span>, <span class="string">&quot;mrpc&quot;</span>)</span><br><span class="line">checkpoint = <span class="string">&quot;bert-base-uncased&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize_function</span>(<span class="params">example</span>):</span><br><span class="line">    <span class="keyword">return</span> tokenizer(example[<span class="string">&quot;sentence1&quot;</span>], example[<span class="string">&quot;sentence2&quot;</span>], truncation=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tokenized_datasets = raw_datasets.<span class="built_in">map</span>(tokenize_function, batched=<span class="literal">True</span>)</span><br><span class="line">data_collator = DataCollatorWithPadding(tokenizer=tokenizer)</span><br></pre></td></tr></table></figure>


<h2 id="3-1-训练"><a href="#3-1-训练" class="headerlink" title="3.1 训练"></a>3.1 训练</h2><p>定义<code>Trainer</code>之前的第一步是定义一个<code>TrainingArguments</code>类，该类将包含<code>Trainer</code>将用于训练和评估的所有超参数。您必须提供的唯一参数是保存训练模型的目录，以及沿途的检查点。对于所有其他作，您可以保留默认值，这应该可以很好地进行基本的微调。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> TrainingArguments</span><br><span class="line">training_args = TrainingArguments(<span class="string">&quot;test-trainer&quot;</span>)</span><br></pre></td></tr></table></figure>


<blockquote>
<p>💡 如果要在训练期间自动将模型上传到中心，请在<code>TrainingArguments</code>中传递<code>push_to_hub=True</code>。我们将在<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter4/3" title="第 4 章">第 4 章</a>中了解更多信息</p>
</blockquote>
<p>第二步是定义我们的模型。与<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter2" title="上一章">上一章</a>一样，我们将使用具有两个标签的<code>AutoModelForSequenceClassification</code>类：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification</span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>


<p>您会注意到，与<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter2" title="第 2 章">第 2 章</a>不同，在实例化此预训练模型后，您会收到一条警告。这是因为 BERT 尚未对句子对进行分类进行预训练，因此预训练模型的头部已被丢弃，而是添加了适合序列分类的新头部。警告表示某些权重未使用（与放置的预训练头对应的权重），并且其他一些权重是随机初始化的（新头的权重）。最后，它鼓励您训练模型，这正是我们现在要做的事情。</p>
<p>一旦有了模型，我们就可以定义一个<code>Trainer</code>，方法是将到目前为止构建的所有对象（<code>模型</code>、<code>training_args</code>、训练和验证数据集、<code>data_collator</code>和<code>分词器</code>）传递给它：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model,</span><br><span class="line">    training_args,</span><br><span class="line">    train_dataset=tokenized_datasets[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">    eval_dataset=tokenized_datasets[<span class="string">&quot;validation&quot;</span>],</span><br><span class="line">    data_collator=data_collator,</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>


<p>请注意，当您像我们在此处所做的那样传递<code>分词器</code>时，<code>Trainer</code>使用的默认<code>data_collator</code>将是之前定义的<code>DataCollatorWithPadding</code>，因此您可以在此调用中跳过<code>data_collator=data_collator</code>行。在第 2 节中向您展示这部分处理仍然很重要！</p>
<p>要在我们的数据集上微调模型，我们只需要调用<code>Trainer</code>的<code>train()</code>方法：</p>
<p>这将开始微调（在 GPU 上应该需要几分钟）并每 500 步报告一次训练损失。但是，它不会告诉您模型的性能如何（或差）。这是因为：</p>
<ol>
<li>我们没有告诉<code>Trainer</code>在训练期间进行评估，而是将<code>TrainingArguments</code>中的<code>eval_strategy</code>设置为<code>“steps”</code>（每<code>eval_steps</code>评估一次）或<code>“epoch”</code>（在每个 epoch 结束时评估）。</li>
<li>我们没有为<code>Trainer</code>提供<code>compute_metrics()</code>函数来计算所述评估期间的指标（否则评估只会打印损失，这不是一个非常直观的数字）。</li>
</ol>
<h2 id="3-2-评估"><a href="#3-2-评估" class="headerlink" title="3.2 评估"></a>3.2 评估</h2><p>让我们看看如何构建一个有用的<code>compute_metrics()</code>函数并在下次训练时使用它。该函数必须采用<code>EvalPrediction</code>对象（该对象是具有<code>predictions</code>字段和<code>label_ids</code>字段的命名元组），并将返回将字符串映射到浮点数的字典（字符串是返回的指标的名称，浮点数是其值）。要从我们的模型中获得一些预测，我们可以使用<code>Trainer.predict()</code>命令：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predictions = trainer.predict(tokenized_datasets[<span class="string">&quot;validation&quot;</span>])</span><br><span class="line"><span class="built_in">print</span>(predictions.predictions.shape, predictions.label_ids.shape)</span><br></pre></td></tr></table></figure>


<p><code>predict()</code>方法的输出是另一个命名元组，其中包含三个字段：<code>predictions</code>、<code>label_ids</code>和<code>metrics</code>。<code>metrics</code>字段将仅包含所传递数据集的损失，以及一些时间指标（预测总时间和平均值）。完成<code>compute_metrics()</code>函数并将其传递给<code>Trainer</code>后，该字段还将包含<code>compute_metrics()</code>返回的指标。</p>
<p>如您所见，<code>predictions</code>是一个形状为 408 x 2 的二维数组（408 是我们使用的数据集中的元素数）。这些是我们传递给<code>predict()</code>的数据集的每个元素的 logits（正如您在<a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter2" title="上一章">上一章</a>中看到的，所有 Transformer 模型都返回 logits）。要将它们转换为我们可以与标签进行比较的预测，我们需要在第二个轴上获取具有最大值的索引：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">preds = np.argmax(predictions.predictions, axis=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>


<p>我们现在可以将这些<code>preds</code>与标签进行比较。为了构建我们的<code>compute_metric()</code>函数，我们将依赖<a target="_blank" rel="noopener" href="https://github.com/huggingface/evaluate/" title="Evaluate">Evaluate</a>库中的🤗指标。我们可以像加载数据集一样轻松地加载与 MRPC 数据集相关的指标，这次使用<code>evaluate.load()</code>函数。返回的对象有一个<code>table()</code>方法，我们可以使用它来执行度量计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> evaluate</span><br><span class="line"></span><br><span class="line">metric = evaluate.load(<span class="string">&quot;glue&quot;</span>, <span class="string">&quot;mrpc&quot;</span>)</span><br><span class="line">metric.compute(predictions=preds, refe  rences=predictions.label_ids)</span><br></pre></td></tr></table></figure>


<p>您获得的确切结果可能会有所不同，因为模型头的随机初始化可能会改变它实现的指标。在这里，我们可以看到我们的模型在验证集上的准确率为 85.78%，F1 分数为 89.97。这是用于评估 GLUE 基准测试的 MRPC 数据集结果的两个指标。<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1810.04805.pdf" title="BERT 论文">BERT 论文</a>中的表格报告了基本模型的 F1 分数为 88.9。这是我们目前使用<code>有外壳</code>模型时的<code>无外壳</code>模型，这解释了更好的结果。</p>
<p>将所有内容打包在一起，我们得到<code>compute_metrics()</code>函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_metrics</span>(<span class="params">eval_preds</span>):</span><br><span class="line">    metric = evaluate.load(<span class="string">&quot;glue&quot;</span>, <span class="string">&quot;mrpc&quot;</span>)</span><br><span class="line">    logits, labels = eval_preds</span><br><span class="line">    predictions = np.argmax(logits, axis=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> metric.compute(predictions=predictions, references=labels)</span><br></pre></td></tr></table></figure>


<p>为了看到它在每个 epoch 结束时报告指标的实际效果，以下是我们如何使用此<code>compute_metrics()</code>函数定义新的<code>Trainer</code>：</p>
<p>请注意，我们创建一个新的<code>TrainingArguments</code>，将其<code>eval_strategy</code>设置为<code>“epoch”</code>和一个新模型 — 否则，我们将继续训练我们已经训练过的模型。要启动新的训练运行，我们执行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">trainer.train()</span><br></pre></td></tr></table></figure>


<p>这一次，它将在每个 epoch 结束时报告训练损失之外的验证损失和指标。同样，由于模型的随机头部初始化，您达到的确切准确率&#x2F;F1 分数可能与我们发现的略有不同，但它应该在同一个范围内。</p>
<p><code>Trainer</code>将在多个 GPU 或 TPU 上开箱即用，并提供许多选项，例如混合精度训练（在训练参数中使用<code>fp16=True</code>）。我们将在第 10 章中介绍它支持的所有内容。</p>
<hr>
<h1 id="4-完整的训练过程"><a href="#4-完整的训练过程" class="headerlink" title="4 完整的训练过程"></a>4 完整的训练过程</h1><p>现在，我们将了解如何在不使用<code>Trainer</code>类的情况下获得与上一节相同的结果。同样，我们假设您已经完成了第 2 节中的数据处理。这是一个简短的摘要，涵盖了您需要的一切：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, DataCollatorWithPadding</span><br><span class="line"></span><br><span class="line">raw_datasets = load_dataset(<span class="string">&quot;glue&quot;</span>, <span class="string">&quot;mrpc&quot;</span>)</span><br><span class="line">checkpoint = <span class="string">&quot;bert-base-uncased&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize_function</span>(<span class="params">example</span>):</span><br><span class="line">    <span class="keyword">return</span> tokenizer(example[<span class="string">&quot;sentence1&quot;</span>], example[<span class="string">&quot;sentence2&quot;</span>], truncation=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tokenized_datasets = raw_datasets.<span class="built_in">map</span>(tokenize_function, batched=<span class="literal">True</span>)</span><br><span class="line">data_collator = DataCollatorWithPadding(tokenizer=tokenizer)</span><br></pre></td></tr></table></figure>


<h2 id="4-1-数据处理"><a href="#4-1-数据处理" class="headerlink" title="4.1 数据处理"></a>4.1 数据处理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">tokenized_datasets = tokenized_datasets.remove_columns([<span class="string">&quot;sentence1&quot;</span>, <span class="string">&quot;sentence2&quot;</span>, <span class="string">&quot;idx&quot;</span>])</span><br><span class="line">tokenized_datasets = tokenized_datasets.rename_column(<span class="string">&quot;label&quot;</span>, <span class="string">&quot;labels&quot;</span>)</span><br><span class="line">tokenized_datasets.set_format(<span class="string">&quot;torch&quot;</span>)</span><br><span class="line">tokenized_datasets[<span class="string">&quot;train&quot;</span>].column_names</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 dataloaders</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_dataloader = DataLoader(</span><br><span class="line">    tokenized_datasets[<span class="string">&quot;train&quot;</span>], shuffle=<span class="literal">True</span>, batch_size=<span class="number">8</span>, collate_fn=data_collator</span><br><span class="line">)</span><br><span class="line">eval_dataloader = DataLoader(</span><br><span class="line">    tokenized_datasets[<span class="string">&quot;validation&quot;</span>], batch_size=<span class="number">8</span>, collate_fn=data_collator</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>


<ul>
<li>获取批量数据</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">&#123;k: v.shape <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br></pre></td></tr></table></figure>


<h2 id="4-2-模型实现"><a href="#4-2-模型实现" class="headerlink" title="4.2 模型实现"></a>4.2 模型实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification</span><br><span class="line"></span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>


<p>4.3 优化器和学习率调度器</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> AdamW</span><br><span class="line"></span><br><span class="line">optimizer = AdamW(model.parameters(), lr=<span class="number">5e-5</span>)</span><br></pre></td></tr></table></figure>


<p>默认使用的学习率调度器只是从最大值 （5e-5） 到 0 的线性衰减。为了正确定义它，我们需要知道我们将采取的训练步骤数，即我们想要运行的 epoch 数乘以训练批次数（即我们的训练数据加载器的长度）。<code>Trainer</code>默认使用三个 epoch，因此我们将遵循：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> get_scheduler</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">num_training_steps = num_epochs * <span class="built_in">len</span>(train_dataloader)</span><br><span class="line">lr_scheduler = get_scheduler(</span><br><span class="line">    <span class="string">&quot;linear&quot;</span>,</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    num_warmup_steps=<span class="number">0</span>,</span><br><span class="line">    num_training_steps=num_training_steps,</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(num_training_steps)</span><br></pre></td></tr></table></figure>


<h2 id="4-4-训练循环"><a href="#4-4-训练循环" class="headerlink" title="4.4 训练循环"></a>4.4 训练循环</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span>) <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> torch.device(<span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)</span><br><span class="line">device</span><br></pre></td></tr></table></figure>


<p>添加进度条：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line">progress_bar = tqdm(<span class="built_in">range</span>(num_training_steps))</span><br><span class="line"></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        batch = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br><span class="line">        outputs = model(**batch)</span><br><span class="line">        loss = outputs.loss</span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        optimizer.step()</span><br><span class="line">        lr_scheduler.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        progress_bar.update(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>


<h2 id="4-5-评估模型"><a href="#4-5-评估模型" class="headerlink" title="4.5 评估模型"></a>4.5 评估模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> evaluate</span><br><span class="line"></span><br><span class="line">metric = evaluate.load(<span class="string">&quot;glue&quot;</span>, <span class="string">&quot;mrpc&quot;</span>)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> eval_dataloader:</span><br><span class="line">    batch = &#123;k: v.to(device) <span class="keyword">for</span> k, v <span class="keyword">in</span> batch.items()&#125;</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**batch)</span><br><span class="line"></span><br><span class="line">    logits = outputs.logits</span><br><span class="line">    predictions = torch.argmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">    metric.add_batch(predictions=predictions, references=batch[<span class="string">&quot;labels&quot;</span>])</span><br><span class="line"></span><br><span class="line">metric.compute()</span><br></pre></td></tr></table></figure>


<h2 id="4-6-使用Accelerate进行加速"><a href="#4-6-使用Accelerate进行加速" class="headerlink" title="4.6 使用Accelerate进行加速"></a>4.6 使用Accelerate进行加速</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> accelerate <span class="keyword">import</span> Accelerator</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> AdamW</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification, get_scheduler</span><br><span class="line"></span><br><span class="line">accelerator = Accelerator()</span><br><span class="line"></span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=<span class="number">2</span>)</span><br><span class="line">optimizer = AdamW(model.parameters(), lr=<span class="number">3e-5</span>)</span><br><span class="line"></span><br><span class="line">train_dl, eval_dl, model, optimizer = accelerator.prepare(</span><br><span class="line">    train_dataloader, eval_dataloader, model, optimizer</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">num_training_steps = num_epochs * <span class="built_in">len</span>(train_dl)</span><br><span class="line">lr_scheduler = get_scheduler(</span><br><span class="line">    <span class="string">&quot;linear&quot;</span>,</span><br><span class="line">    optimizer=optimizer,</span><br><span class="line">    num_warmup_steps=<span class="number">0</span>,</span><br><span class="line">    num_training_steps=num_training_steps,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">progress_bar = tqdm(<span class="built_in">range</span>(num_training_steps))</span><br><span class="line"></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_dl:</span><br><span class="line">        outputs = model(**batch)</span><br><span class="line">        loss = outputs.loss</span><br><span class="line">        accelerator.backward(loss)</span><br><span class="line"></span><br><span class="line">        optimizer.step()</span><br><span class="line">        lr_scheduler.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        progress_bar.update(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://zhouzimu.top">弘树</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://zhouzimu.top/2025/05/01/3-%E5%BE%AE%E8%B0%83%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/">http://zhouzimu.top/2025/05/01/3-微调预训练模型/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://zhouzimu.top" target="_blank">诺亚方舟</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post_share"><div class="social-share" data-image="https://venturebeat.com/wp-content/uploads/2023/05/Untitled-design-78.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>码字不易，如果对你有帮助的话请喝一杯奶茶吧~</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.png" target="_blank"><img class="post-qr-code-img" src="/img/wechat.png" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.png" target="_blank"><img class="post-qr-code-img" src="/img/alipay.png" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/05/02/5-%E6%95%B0%E6%8D%AE%E9%9B%86/" title="「HFLLM」5-数据集"><img class="cover" src="https://cdn.mos.cms.futurecdn.net/gHfBJ6FBHKnLbW36hEDvgV.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">「HFLLM」5-数据集</div></div></a></div><div class="next-post pull-right"><a href="/2025/02/17/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB9%EF%BC%9AADSNet/" title="论文精读9：ADSNet"><img class="cover" src="https://i.ytimg.com/vi/v5BoeTyOAhM/hq720.jpg?sqp=-oaymwE7CK4FEIIDSFryq4qpAy0IARUAAAAAGAElAADIQj0AgKJD8AEB-AH-CYACzgWKAgwIABABGFsgYShlMA8=&amp;rs=AOn4CLChXkjgk_egO-1uGInclI_lQe_MMg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">论文精读9：ADSNet</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2025/05/02/5-%E6%95%B0%E6%8D%AE%E9%9B%86/" title="「HFLLM」5-数据集"><img class="cover" src="https://cdn.mos.cms.futurecdn.net/gHfBJ6FBHKnLbW36hEDvgV.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-02</div><div class="title">「HFLLM」5-数据集</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="utterances-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/pic_editor_1635545191.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">弘树</div><div class="author-info__description">在我坚定无比的内心，总以最坚强的节奏解开案情</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">123</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">45</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">17</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/NoyeArk"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/NoyeArk" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:horiki0@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a><a class="social-icon" href="https://leetcode.cn/u/horiki/" target="_blank" title="Leetcode"><i class="fas fa-l" style="color: #59e285;"></i></a><a class="social-icon" href="https://www.kaggle.com/horiki" target="_blank" title="Kaggle"><i class="fas fa-k" style="color: #4a7dbe;"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc" style="font-size: 15px;"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E4%BB%8B%E7%BB%8D"><span class="toc-text">1 介绍</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE"><span class="toc-text">2 处理数据</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E4%BB%8EHub%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">2.1 从Hub加载数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E9%A2%84%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">2.2 预处理数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-%E5%8A%A8%E6%80%81%E5%A1%AB%E5%85%85"><span class="toc-text">2.3 动态填充</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%E4%BD%BF%E7%94%A8Trainer-API%E5%BE%AE%E8%B0%83%E6%A8%A1%E5%9E%8B"><span class="toc-text">3 使用Trainer API微调模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E8%AE%AD%E7%BB%83"><span class="toc-text">3.1 训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E8%AF%84%E4%BC%B0"><span class="toc-text">3.2 评估</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-%E5%AE%8C%E6%95%B4%E7%9A%84%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-text">4 完整的训练过程</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86"><span class="toc-text">4.1 数据处理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E6%A8%A1%E5%9E%8B%E5%AE%9E%E7%8E%B0"><span class="toc-text">4.2 模型实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-4-%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF"><span class="toc-text">4.4 训练循环</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-5-%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%9E%8B"><span class="toc-text">4.5 评估模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-6-%E4%BD%BF%E7%94%A8Accelerate%E8%BF%9B%E8%A1%8C%E5%8A%A0%E9%80%9F"><span class="toc-text">4.6 使用Accelerate进行加速</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline" style="font-size: 17px;"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list" style="font-size: 16px;"><div class="aside-list-item"><a class="thumbnail" href="/2025/05/02/5-%E6%95%B0%E6%8D%AE%E9%9B%86/" title="「HFLLM」5-数据集"><img src="https://cdn.mos.cms.futurecdn.net/gHfBJ6FBHKnLbW36hEDvgV.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「HFLLM」5-数据集"/></a><div class="content"><a class="title" href="/2025/05/02/5-%E6%95%B0%E6%8D%AE%E9%9B%86/" title="「HFLLM」5-数据集">「HFLLM」5-数据集</a><time datetime="2025-05-02T08:26:25.000Z" title="发表于 2025-05-02 16:26:25">2025-05-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/05/01/3-%E5%BE%AE%E8%B0%83%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/" title="「HFLLM」3. 微调预训练模型"><img src="https://venturebeat.com/wp-content/uploads/2023/05/Untitled-design-78.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="「HFLLM」3. 微调预训练模型"/></a><div class="content"><a class="title" href="/2025/05/01/3-%E5%BE%AE%E8%B0%83%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/" title="「HFLLM」3. 微调预训练模型">「HFLLM」3. 微调预训练模型</a><time datetime="2025-05-01T02:51:22.000Z" title="发表于 2025-05-01 10:51:22">2025-05-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/17/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB9%EF%BC%9AADSNet/" title="论文精读9：ADSNet"><img src="https://i.ytimg.com/vi/v5BoeTyOAhM/hq720.jpg?sqp=-oaymwE7CK4FEIIDSFryq4qpAy0IARUAAAAAGAElAADIQj0AgKJD8AEB-AH-CYACzgWKAgwIABABGFsgYShlMA8=&amp;rs=AOn4CLChXkjgk_egO-1uGInclI_lQe_MMg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文精读9：ADSNet"/></a><div class="content"><a class="title" href="/2025/02/17/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB9%EF%BC%9AADSNet/" title="论文精读9：ADSNet">论文精读9：ADSNet</a><time datetime="2025-02-17T09:09:52.000Z" title="发表于 2025-02-17 17:09:52">2025-02-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/01/27/2024%E5%B9%B4AI%E5%B9%B4%E5%BA%A6%E5%85%B3%E9%94%AE%E8%AF%8D/" title="2024年AI年度关键词"><img src="https://image.uisdc.com/wp-content/uploads/2025/01/banner2025012108482456.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="2024年AI年度关键词"/></a><div class="content"><a class="title" href="/2025/01/27/2024%E5%B9%B4AI%E5%B9%B4%E5%BA%A6%E5%85%B3%E9%94%AE%E8%AF%8D/" title="2024年AI年度关键词">2024年AI年度关键词</a><time datetime="2025-01-27T11:03:16.000Z" title="发表于 2025-01-27 19:03:16">2025-01-27</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://venturebeat.com/wp-content/uploads/2023/05/Untitled-design-78.png')"><div id="footer-wrap" style="padding: 5px 5px;"><div class="copyright">&copy;2024 - 2025 By 弘树</div><div id="running-time" style="font-size: 14px;"><script>setInterval(() => {
  let create_time = Math.round((new Date(2024, 1, 23).getTime()) / 1000);
  let timestamp = Math.round((new Date().getTime()) / 1000);
  let second = timestamp - create_time;
  let time = new Array(0, 0, 0, 0, 0);
  if (second >= 365 * 24 * 3600) {
      time[0] = parseInt(second / (365 * 24 * 3600));
      second %= 365 * 24 * 3600;
  }
  if (second >= 24 * 3600) {
      time[1] = parseInt(second / (24 * 3600));
      second %= 24 * 3600;
  }
  if (second >= 3600) {
      time[2] = parseInt(second / 3600);
      second %= 3600;
  }
  if (second >= 60) {
      time[3] = parseInt(second / 60);
      second %= 60;
  }
  if (second > 0) {
      time[4] = second;
  }
  currentTimeHtml = "本站已安全运行 " +
      time[0] + " 年 " +
      (time[1] + 31) + " 天 " +
      time[2] + " 时 " +
      time[3] + " 分 " +
      time[4] + " 秒";
  document.getElementById("running-time").innerHTML = currentTimeHtml;
  }, 1000);</script></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>(() => {
  const loadUtterances = () => {
    let ele = document.createElement('script')
    ele.id = 'utterances_comment'
    ele.src = 'https://utteranc.es/client.js'
    ele.setAttribute('repo', 'NoyeArk/noyeark.github.io')
    ele.setAttribute('issue-term', 'pathname')
    const nowTheme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'photon-dark' : 'github-light'
    ele.setAttribute('theme', nowTheme)
    ele.crossOrigin = 'anonymous'
    ele.async = true
    document.getElementById('utterances-wrap').appendChild(ele)
  }

  const utterancesTheme = theme => {
    const iframe = document.querySelector('.utterances-frame')
    if (iframe) {
      const theme = theme === 'dark' ? 'photon-dark' : 'github-light'
      const message = {
        type: 'set-theme',
        theme: theme
      };
      iframe.contentWindow.postMessage(message, 'https://utteranc.es');
    }
  }

  btf.addGlobalFn('themeChange', utterancesTheme, 'utterances')

  if ('Utterances' === 'Utterances' || !false) {
    if (false) btf.loadComment(document.getElementById('utterances-wrap'), loadUtterances)
    else loadUtterances()
  } else {
    window.loadOtherComment = loadUtterances
  }
})()</script></div><script src="/js/jquery.js"></script><script src="/js/footer.js"></script><script src="/js/nav.js"></script><script>let tianliGPT_postSelector = '\#post \#article-container';let tianliGPT_key = 'iyiSf8ljbaf';</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.12.0"></script></div></div></body></html>